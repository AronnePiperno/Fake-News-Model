{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56s6Bp6a-It1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
<<<<<<< HEAD
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
=======
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
<<<<<<< HEAD
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n"
=======
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext.vocab as vocab\n",
        "from torch.autograd import Variable\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 13,
=======
      "execution_count": 2,
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5Ibwe3D_aoc",
<<<<<<< HEAD
        "outputId": "94df76e3-8983-4611-ddb5-47df1b285097"
=======
        "outputId": "857c0126-5147-4f2a-d101-396b6584f003"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.0\n",
            "    Uninstalling gdown-4.7.0:\n",
            "      Successfully uninstalled gdown-4.7.0\n",
            "Successfully installed gdown-4.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRetrieving folder list\n",
=======
            "/bin/bash: line 1: conda: command not found\n",
            "Retrieving folder list\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
            "Processing file 1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C README\n",
            "Processing file 1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG test.tsv\n",
            "Processing file 1RvXY274ln1cKZR6LaUacyytgrrxNqH3K train.tsv\n",
            "Processing file 1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi valid.tsv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C\n",
            "To: /content/liar_dataset/README\n",
<<<<<<< HEAD
            "100% 1.67k/1.67k [00:00<00:00, 8.07MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG\n",
            "To: /content/liar_dataset/test.tsv\n",
            "100% 301k/301k [00:00<00:00, 79.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RvXY274ln1cKZR6LaUacyytgrrxNqH3K\n",
            "To: /content/liar_dataset/train.tsv\n",
            "100% 2.41M/2.41M [00:00<00:00, 220MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi\n",
            "To: /content/liar_dataset/valid.tsv\n",
            "100% 302k/302k [00:00<00:00, 138MB/s]\n",
=======
            "100% 1.67k/1.67k [00:00<00:00, 6.30MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG\n",
            "To: /content/liar_dataset/test.tsv\n",
            "100% 301k/301k [00:00<00:00, 95.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RvXY274ln1cKZR6LaUacyytgrrxNqH3K\n",
            "To: /content/liar_dataset/train.tsv\n",
            "100% 2.41M/2.41M [00:00<00:00, 273MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi\n",
            "To: /content/liar_dataset/valid.tsv\n",
            "100% 302k/302k [00:00<00:00, 132MB/s]\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
            "Download completed\n"
          ]
        }
      ],
      "source": [
<<<<<<< HEAD
        "!pip install gdown==4.6.0\n",
        "!gdown --folder https://drive.google.com/drive/u/1/folders/15Wn46r7gidaiZbx2ArFYsd7rjYH4y7JM"
=======
        "!conda install -y gdown\n",
        "\n",
        "!gdown --folder 15Wn46r7gidaiZbx2ArFYsd7rjYH4y7JM"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfQhIlP-It4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#open the csv files\n",
        "csv_path_fake = os.path.join( './', 'News_dataset', 'Fake.csv')\n",
        "csv_path_true = os.path.join( './', 'News_dataset', 'True.csv')\n",
        "\n",
        "df_fake = pd.read_csv(csv_path_fake)\n",
        "df_true = pd.read_csv(csv_path_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fta4TO32-It5"
      },
      "outputs": [],
      "source": [
        "#add a label column to the dataframes\n",
        "df_fake['label'] = 'fake'\n",
        "df_true['label'] = 'true'\n",
        "\n",
        "df = pd.concat([df_fake, df_true], ignore_index=True)\n",
        "\n",
        "#save the dataframe to a csv file\n",
        "df.to_csv('news.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAtJy4W9-It6"
      },
      "outputs": [],
      "source": [
        "# open the csv file with pandas\n",
        "\n",
        "df = pd.read_csv('news.csv')\n",
        "\n",
        "#split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.1, random_state=7)\n",
        "\n",
        "#initialize a TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
        "\n",
        "#fit and transform train set, transform test set\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
<<<<<<< HEAD
        "id": "o3UCbrrk-lAA"
      },
      "outputs": [],
      "source": [
        "def calculate_and_save_metrics(y_test, y_pred, model_name, label, first = False, last = False, results = None, savename = ''):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print('Accuracy: ', accuracy)\n",
        "    print('Precision: ', precision)\n",
        "    print('Recall: ', recall)\n",
        "    print('F1: ', f1)\n",
        "\n",
        "    print(confusion_matrix(y_test, y_pred, labels=label))\n",
        "\n",
        "    if first:\n",
        "        results = pd.DataFrame([[accuracy, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=[model_name])\n",
        "    else:\n",
        "        results.loc[model_name] = [accuracy, precision, recall, f1]\n",
        "\n",
        "    if last:\n",
        "        results.to_csv(savename)\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
=======
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "id": "lycTNkCk-It6",
        "outputId": "2746725c-21bc-4257-db15-199ab02291c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.9944320712694877\n",
            "Precision:  0.9944339701357502\n",
            "Recall:  0.9944320712694877\n",
            "F1:  0.9944317848085675\n",
            "[[2336   10]\n",
            " [  15 2129]]\n"
          ]
        }
      ],
      "source": [
        "label = ['fake', 'true']\n",
        "\n",
=======
            "Accuracy: 99.38%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2335,   11],\n",
              "       [  17, 2127]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "#initialize a PassiveAggressiveClassifier\n",
        "pac = PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train, y_train)\n",
        "\n",
        "#predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'PassiveAggressiveClassifier', label, True)"
=======
        "score = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(y_test, y_pred, labels=['fake', 'true'])\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTvt27XZ-It7",
        "outputId": "1644ccbe-72c7-47e2-b61e-0493215ce07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.9866369710467706\n",
            "Precision:  0.9866368213705528\n",
            "Recall:  0.9866369710467706\n",
            "F1:  0.9866367000464401\n",
            "[[2317   29]\n",
            " [  31 2113]]\n"
          ]
=======
            "Accuracy: 98.66%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2317,   29],\n",
              "       [  31, 2113]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "#with logistic regression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(tfidf_train, y_train)\n",
        "y_pred = logreg.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'LogisticRegression', label, results=results)"
=======
        "score = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(y_test, y_pred, labels=['fake', 'true'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwT5Zwgf-It8",
        "outputId": "f5433594-0f6a-4549-87b5-f44ef84fd90a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.9396436525612473\n",
            "Precision:  0.939656760061686\n",
            "Recall:  0.9396436525612473\n",
            "F1:  0.9396295976850317\n",
            "[[2221  125]\n",
            " [ 146 1998]]\n"
          ]
=======
            "Accuracy: 93.96%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2221,  125],\n",
              "       [ 146, 1998]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "#with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, y_train)\n",
        "y_pred = nb.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9zHKhft-lAC",
        "outputId": "69fdb5cf-714f-4951-c6b4-1e70c827c19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6561247216035635\n",
            "Precision:  0.773658716218475\n",
            "Recall:  0.6561247216035635\n",
            "F1:  0.6060047436568158\n",
            "[[2319   27]\n",
            " [1517  627]]\n"
          ]
        }
      ],
      "source": [
        "# with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, y_train)\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'KNeighborsClassifier', label=label, results=results)\n"
=======
        "score = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(y_test, y_pred, labels=['fake', 'true'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YodjLwAV-It8",
        "outputId": "a4bf0ac3-4685-4d61-866b-04fabd67379f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.9904231625835189\n",
            "Precision:  0.9904265610194284\n",
            "Recall:  0.9904231625835189\n",
            "F1:  0.9904236313996031\n",
            "[[2322   24]\n",
            " [  19 2125]]\n"
          ]
=======
            "Accuracy: 98.89%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2324,   22],\n",
              "       [  28, 2116]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, y_train)\n",
        "y_pred = rf.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'RandomForestClassifier', label=label, results=results)"
=======
        "score = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(y_test, y_pred, labels=['fake', 'true'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5nlj2uo-It9",
        "outputId": "68101f81-a5a7-4023-8f98-8c8f65e7b4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.9946547884187082\n",
            "Precision:  0.9946576855359367\n",
            "Recall:  0.9946547884187082\n",
            "F1:  0.9946544568146701\n",
            "[[2337    9]\n",
            " [  15 2129]]\n"
          ]
=======
            "Accuracy: 99.47%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2337,    9],\n",
              "       [  15, 2129]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, y_train)\n",
        "y_pred = svc.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_kaggle.csv')"
=======
        "score = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(y_test, y_pred, labels=['fake', 'true'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 3,
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      "metadata": {
        "id": "H69Tw1N6-It9"
      },
      "outputs": [],
      "source": [
        "# using Liar dataset\n",
        "\n",
<<<<<<< HEAD
        "csv_path_liar_train = os.path.join( './', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( './', 'liar_dataset', 'test.tsv')\n",
=======
        "csv_path_liar_train = os.path.join( '/content', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( '/content', 'liar_dataset', 'test.tsv')\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
        "\n",
        "#fit and transform train set, transform test set\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(df_liar_train['statement'])\n",
<<<<<<< HEAD
        "tfidf_test = tfidf_vectorizer.transform(df_liar_test['statement'])\n",
        "\n",
        "label = ['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire']"
=======
        "tfidf_test = tfidf_vectorizer.transform(df_liar_test['statement'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5MO9JQh-It-",
        "outputId": "504b152b-a6f2-44bc-cb4c-75887e24daa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.23362273086029992\n",
            "Precision:  0.23245313772747705\n",
            "Recall:  0.23362273086029992\n",
            "F1:  0.2323593773721207\n",
            "[[43 38 46 20 52  9]\n",
            " [50 52 50 37 37 15]\n",
            " [50 56 58 40 40 21]\n",
            " [33 45 29 38 46 21]\n",
            " [36 49 34 34 86 10]\n",
            " [13 10 15 15 20 19]]\n"
          ]
        }
      ],
      "source": [
        "#initialize a PassiveAggressiveClassifier\n",
        "\n",
=======
            "Accuracy: 22.81%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[43, 38, 49, 22, 47,  9],\n",
              "       [49, 51, 52, 36, 38, 15],\n",
              "       [50, 58, 59, 39, 37, 22],\n",
              "       [36, 43, 31, 40, 41, 21],\n",
              "       [38, 49, 38, 36, 74, 14],\n",
              "       [14, 11, 13, 15, 17, 22]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "#initialize a PassiveAggressiveClassifier\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "pac = PassiveAggressiveClassifier(max_iter=1000)\n",
        "pac.fit(tfidf_train, df_liar_train['label'])\n",
        "\n",
        "#predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'PassiveAggressiveClassifier', label, first=True)"
=======
        "score = accuracy_score(df_liar_test['label'], y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(df_liar_test['label'], y_pred, labels=['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'])\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryaxdMxP-It-",
        "outputId": "9ce7a26d-15dd-4544-baaf-309a1b7f909f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.250197316495659\n",
            "Precision:  0.24541341568325675\n",
            "Recall:  0.250197316495659\n",
            "F1:  0.24157143838446754\n",
            "[[47 50 46 13 50  2]\n",
            " [50 56 65 25 44  1]\n",
            " [28 65 84 37 47  4]\n",
            " [22 37 53 44 55  1]\n",
            " [29 50 57 26 84  3]\n",
            " [ 7 14 25 13 31  2]]\n"
          ]
=======
            "Accuracy: 25.02%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[47, 50, 46, 13, 50,  2],\n",
              "       [50, 56, 65, 25, 44,  1],\n",
              "       [28, 65, 84, 37, 47,  4],\n",
              "       [22, 37, 53, 44, 55,  1],\n",
              "       [29, 50, 57, 26, 84,  3],\n",
              "       [ 7, 14, 25, 13, 31,  2]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with logistic regression\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = logreg.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LogisticRegression', label, results=results)\n"
=======
        "score = accuracy_score(df_liar_test['label'], y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(df_liar_test['label'], y_pred, labels=['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'])\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j08XvPQQ-It-",
        "outputId": "cae9e385-c09a-4280-c023-5e4bd47b389d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.23756906077348067\n",
            "Precision:  0.2308894522430316\n",
            "Recall:  0.23756906077348067\n",
            "F1:  0.20623444528178886\n",
            "[[ 15  59  79   3  52   0]\n",
            " [ 14  65 115   7  40   0]\n",
            " [  7  68 126  17  47   0]\n",
            " [  8  37  93  18  56   0]\n",
            " [ 12  51  99  10  77   0]\n",
            " [  2  18  36   7  29   0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/def/miniconda3/envs/big_data/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
=======
            "Accuracy: 23.76%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 15,  59,  79,   3,  52,   0],\n",
              "       [ 14,  65, 115,   7,  40,   0],\n",
              "       [  7,  68, 126,  17,  47,   0],\n",
              "       [  8,  37,  93,  18,  56,   0],\n",
              "       [ 12,  51,  99,  10,  77,   0],\n",
              "       [  2,  18,  36,   7,  29,   0]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = nb.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFzFZVEs-lAF",
        "outputId": "e978c4f7-8e16-4f49-d332-d325e59a449a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.21862667719021311\n",
            "Precision:  0.2148326623861786\n",
            "Recall:  0.21862667719021311\n",
            "F1:  0.2133969072351237\n",
            "[[31 48 39 33 48  9]\n",
            " [34 46 56 39 59  7]\n",
            " [24 49 64 59 63  6]\n",
            " [18 29 41 54 61  9]\n",
            " [26 38 52 46 76 11]\n",
            " [ 9  6 19 25 27  6]]\n"
          ]
        }
      ],
      "source": [
        "# with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'KNeighborsClassifier', label=label, results=results)"
=======
        "score = accuracy_score(df_liar_test['label'], y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(df_liar_test['label'], y_pred, labels=['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTb-nYk-It_",
        "outputId": "eae90176-dd14-4c71-a95e-cfd4df0cb99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.2565114443567482\n",
            "Precision:  0.26151832390741203\n",
            "Recall:  0.2565114443567482\n",
            "F1:  0.24337528869577962\n",
            "[[ 28  61  37  12  68   2]\n",
            " [ 32  72  56  20  60   1]\n",
            " [ 21  69  81  15  75   4]\n",
            " [ 17  35  44  36  80   0]\n",
            " [ 18  40  56  26 103   6]\n",
            " [  6  14  11  11  45   5]]\n"
          ]
=======
            "Accuracy: 24.63%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 23,  61,  40,  15,  67,   2],\n",
              "       [ 35,  76,  48,  26,  55,   1],\n",
              "       [ 18,  76,  70,  21,  77,   3],\n",
              "       [ 15,  46,  42,  30,  76,   3],\n",
              "       [ 20,  43,  48,  26, 109,   3],\n",
              "       [ 10,  10,  14,  13,  41,   4]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = rf.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'RandomForestClassifier', label=label, results=results)"
=======
        "score = accuracy_score(df_liar_test['label'], y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(df_liar_test['label'], y_pred, labels=['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'])"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEqTdG4d-It_",
        "outputId": "d0fafd72-96d4-45f1-ac95-3cfb39b7bd05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Accuracy:  0.24546172059984214\n",
            "Precision:  0.24380461077723303\n",
            "Recall:  0.24546172059984214\n",
            "F1:  0.24410137719093702\n",
            "[[53 44 51 15 38  7]\n",
            " [52 49 53 36 42  9]\n",
            " [31 68 67 51 38 10]\n",
            " [24 31 47 56 44 10]\n",
            " [35 47 46 30 76 15]\n",
            " [10 12 21 17 22 10]]\n"
          ]
=======
            "Accuracy: 24.55%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[53, 44, 51, 15, 38,  7],\n",
              "       [52, 49, 53, 36, 42,  9],\n",
              "       [31, 68, 67, 51, 38, 10],\n",
              "       [24, 31, 47, 56, 44, 10],\n",
              "       [35, 47, 46, 30, 76, 15],\n",
              "       [10, 12, 21, 17, 22, 10]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = svc.predict(tfidf_test)\n",
<<<<<<< HEAD
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_liar.csv')"
=======
        "score = accuracy_score(df_liar_test['label'], y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "\n",
        "#build confusion matrix\n",
        "confusion_matrix(df_liar_test['label'], y_pred, labels=['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeCpGlNR_CA3",
        "outputId": "0dec1ae4-4ba1-4937-ebb6-c2276ed2f71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
<<<<<<< HEAD
      "metadata": {
        "id": "CqW8EHZN-lAF"
      },
      "outputs": [],
      "source": [
        "# label true and mostly-true as true, and false, barely-true and pants-fire as false\n",
        "\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['barely-true'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['barely-true'], 'false')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['pants-fire'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['pants-fire'], 'false')\n",
        "\n",
        "#fit and transform train set, transform test set\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(df_liar_train['statement'])\n",
        "tfidf_test = tfidf_vectorizer.transform(df_liar_test['statement'])\n",
        "\n",
        "label = ['true', 'false']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsjaZJ1J-lAF",
        "outputId": "0baf76a0-3d8e-4c9f-952c-f68a5200dc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4127861089187056\n",
            "Precision:  0.41427441941641197\n",
            "Recall:  0.4127861089187056\n",
            "F1:  0.41300643629512673\n",
            "[[196 151]\n",
            " [192 268]]\n"
          ]
        }
      ],
      "source": [
        "# initialize a PassiveAggressiveClassifier\n",
        "pac = PassiveAggressiveClassifier(max_iter=1000)\n",
        "pac.fit(tfidf_train, df_liar_train['label'])\n",
        "\n",
        "#predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'PassiveAggressiveClassifier', label, first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbZ6c2D4-lAF",
        "outputId": "ac936dc5-144a-41e1-e0d3-34d885175a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4846093133385951\n",
            "Precision:  0.44625772367506816\n",
            "Recall:  0.4846093133385951\n",
            "F1:  0.4455332077814675\n",
            "[[217 213]\n",
            " [149 381]]\n"
          ]
        }
      ],
      "source": [
        "# with logistic regression\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = logreg.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LogisticRegression', label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjKiJaZk-lAG",
        "outputId": "28d51be2-477b-4104-ae65-48c77c4bbb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4996053670086819\n",
            "Precision:  0.603060107348248\n",
            "Recall:  0.4996053670086819\n",
            "F1:  0.4327770131024204\n",
            "[[193 256]\n",
            " [114 439]]\n"
          ]
        }
      ],
      "source": [
        "# with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = nb.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_mJi9UA-lAG",
        "outputId": "d791bb1b-3c2c-4b5c-8848-36f47a3ea142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4506708760852407\n",
            "Precision:  0.4352822744630089\n",
            "Recall:  0.4506708760852407\n",
            "F1:  0.43511527535387057\n",
            "[[164 227]\n",
            " [122 355]]\n"
          ]
        }
      ],
      "source": [
        "# with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'KNeighborsClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqsXs_Hj-lAG",
        "outputId": "09b70470-a3da-42a7-f1e7-e24d8c23e21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.48303078137332284\n",
            "Precision:  0.4685549554923833\n",
            "Recall:  0.48303078137332284\n",
            "F1:  0.4288359724279499\n",
            "[[205 240]\n",
            " [148 401]]\n"
          ]
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = rf.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'RandomForestClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f60smapB-lAG",
        "outputId": "ac088d7f-1689-4b71-e132-4f3b69819058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4554064719810576\n",
            "Precision:  0.43187177447611486\n",
            "Recall:  0.4554064719810576\n",
            "F1:  0.4394366790071621\n",
            "[[213 178]\n",
            " [167 329]]\n"
          ]
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = svc.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_liar_binary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqJEKnOQ-lAG"
      },
      "outputs": [],
      "source": [
        "# WELFake dataset\n",
        "\n",
        "csv_path_welfake = os.path.join( './', 'WELFake', 'WELFake_Dataset.csv')\n",
        "\n",
        "df_welfake = pd.read_csv(csv_path_welfake)\n",
        "\n",
        "# drop the rows with np.nan values on text column\n",
        "df_welfake = df_welfake.dropna(subset=['text'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_welfake['text'], df_welfake['label'], test_size=0.1, random_state=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VexaAFzc-lAG"
      },
      "outputs": [],
      "source": [
        "# initialize a TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
        "\n",
        "# fit and transform train set, transform test set\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "label = [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTYhV-dM-lAG",
        "outputId": "6fb28e27-08a5-40cc-fceb-4c2535c3c466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9588072122052704\n",
            "Precision:  0.9588822640330955\n",
            "Recall:  0.9588072122052704\n",
            "F1:  0.9588004132730461\n",
            "[[3369  173]\n",
            " [ 124 3544]]\n"
          ]
        }
      ],
      "source": [
        "# initialize a PassiveAggressiveClassifier\n",
        "pac = PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train, y_train)\n",
        "\n",
        "# predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'PassiveAggressiveClassifier', label, first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3fHX9AO-lAG",
        "outputId": "b5ebb12c-d29e-482a-cb64-9511ba5564b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9439667128987518\n",
            "Precision:  0.9443510758208267\n",
            "Recall:  0.9439667128987518\n",
            "F1:  0.9439387007893646\n",
            "[[3285  257]\n",
            " [ 147 3521]]\n"
          ]
        }
      ],
      "source": [
        "# with logistic regression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(tfidf_train, y_train)\n",
        "y_pred = logreg.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'LogisticRegression', label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2o8-RoP-lAG",
        "outputId": "0deeea2a-28fb-4f54-8574-9f734f4808a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8690707350901525\n",
            "Precision:  0.8690674737650315\n",
            "Recall:  0.8690707350901525\n",
            "F1:  0.8690680341744951\n",
            "[[3066  476]\n",
            " [ 468 3200]]\n"
          ]
        }
      ],
      "source": [
        "# with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, y_train)\n",
        "y_pred = nb.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2D5fAqP-lAG",
        "outputId": "72467b2a-9e7e-4787-8d3a-ab860b66f1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6300970873786408\n",
            "Precision:  0.750457903727056\n",
            "Recall:  0.6300970873786408\n",
            "F1:  0.5755096553135339\n",
            "[[ 951 2591]\n",
            " [  76 3592]]\n"
          ]
        }
      ],
      "source": [
        "#with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, y_train)\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'KNeighborsClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i34dPuXT-lAH",
        "outputId": "6f096eb4-a2a9-46c4-ed53-dcae611b6812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9360610263522885\n",
            "Precision:  0.936470123059127\n",
            "Recall:  0.9360610263522885\n",
            "F1:  0.9360268995563436\n",
            "[[3254  288]\n",
            " [ 173 3495]]\n"
          ]
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, y_train)\n",
        "y_pred = rf.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'RandomForestClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BRbLdvz-lAH",
        "outputId": "2af1537e-09df-4ba1-ed4a-714f99c4b984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9611650485436893\n",
            "Precision:  0.9613885747423163\n",
            "Recall:  0.9611650485436893\n",
            "F1:  0.9611522961703536\n",
            "[[3361  181]\n",
            " [  99 3569]]\n"
          ]
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, y_train)\n",
        "y_pred = svc.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(y_test, y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_welfake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PokGmAE-lAH"
      },
      "outputs": [],
      "source": [
        "# df liar using statement speaker and party\n",
        "\n",
        "csv_path_liar_train = os.path.join( './', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( './', 'liar_dataset', 'test.tsv')\n",
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "\n",
        "\n",
        "df_liar_train = df_liar_train[['label', 'statement', 'speaker', 'party']]\n",
        "df_liar_test = df_liar_test[['label', 'statement', 'speaker', 'party']]\n",
        "df_liar_train = df_liar_train.dropna()\n",
        "\n",
        "# vectorize the statement column\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(df_liar_train['statement'])\n",
        "tfidf_test = tfidf_vectorizer.transform(df_liar_test['statement'])\n",
        "\n",
        "# vectorize the speaker column\n",
        "tfidf_train_speaker = tfidf_vectorizer.fit_transform(df_liar_train['speaker'])\n",
        "tfidf_test_speaker = tfidf_vectorizer.transform(df_liar_test['speaker'])\n",
        "\n",
        "# vectorize the party column\n",
        "tfidf_train_party = tfidf_vectorizer.fit_transform(df_liar_train['party'])\n",
        "tfidf_test_party = tfidf_vectorizer.transform(df_liar_test['party'])\n",
        "\n",
        "# concatenate the three vectors\n",
        "tfidf_train = np.concatenate((tfidf_train.toarray(), tfidf_train_speaker.toarray(), tfidf_train_party.toarray()), axis=1)\n",
        "tfidf_test = np.concatenate((tfidf_test.toarray(), tfidf_test_speaker.toarray(), tfidf_test_party.toarray()), axis=1)\n",
        "\n",
        "label = ['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d965rtr5-lAH",
        "outputId": "498e4817-a90a-4fa2-e59d-9ccb073f39e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.22888713496448304\n",
            "Precision:  0.23023408526590852\n",
            "Recall:  0.22888713496448304\n",
            "F1:  0.22897047161020762\n",
            "[[43 50 48 18 40  9]\n",
            " [52 52 63 32 38  4]\n",
            " [56 56 66 33 37 17]\n",
            " [29 36 47 41 41 18]\n",
            " [42 41 50 36 64 16]\n",
            " [ 8  8 16 12 24 24]]\n"
          ]
        }
      ],
      "source": [
        "# initialize a PassiveAggressiveClassifier\n",
        "pac = PassiveAggressiveClassifier(max_iter=1000)\n",
        "pac.fit(tfidf_train, df_liar_train['label'])\n",
        "\n",
        "# predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'PassiveAggressiveClassifier', label, first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQPuYTP7-lAH",
        "outputId": "056c75d1-4a11-40e3-cdfa-07a70f38becd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.2557221783741121\n",
            "Precision:  0.2622683610451152\n",
            "Recall:  0.2557221783741121\n",
            "F1:  0.2548912547110119\n",
            "[[60 50 43 15 37  3]\n",
            " [53 64 70 21 31  2]\n",
            " [40 67 68 46 43  1]\n",
            " [25 32 63 35 51  6]\n",
            " [43 37 48 32 77 12]\n",
            " [ 5  9 14 13 31 20]]\n"
          ]
        }
      ],
      "source": [
        "# with logistic regression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = logreg.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LogisticRegression', label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3QQLoeD-lAH",
        "outputId": "715e491e-739c-4b7f-f645-a6a55ec58d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.2549329123914759\n",
            "Precision:  0.26958362898191\n",
            "Recall:  0.2549329123914759\n",
            "F1:  0.24034321415470822\n",
            "[[ 26  70  60   8  42   2]\n",
            " [ 17  81 100  10  32   1]\n",
            " [ 19  67 102  28  49   0]\n",
            " [  9  40  79  20  60   4]\n",
            " [ 19  45  80  19  81   5]\n",
            " [  2  12  18  11  36  13]]\n"
          ]
        }
      ],
      "source": [
        "# with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = nb.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBJvLdlP-lAI",
        "outputId": "05e68c68-39a5-48ed-e502-6bd37a2a714f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.22257300710339384\n",
            "Precision:  0.2314443848997401\n",
            "Recall:  0.22257300710339384\n",
            "F1:  0.22221364309019398\n",
            "[[39 54 38 42 32  3]\n",
            " [31 50 66 50 43  1]\n",
            " [32 53 68 62 46  4]\n",
            " [12 30 35 61 64 10]\n",
            " [27 36 46 78 49 13]\n",
            " [ 3 10 13 22 29 15]]\n"
          ]
        }
      ],
      "source": [
        "# with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'KNeighborsClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBVsA6ZV-lAI",
        "outputId": "bf00214b-706f-49bf-d5b3-e444a49142ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.2462509865824783\n",
            "Precision:  0.25598791463936366\n",
            "Recall:  0.2462509865824783\n",
            "F1:  0.23947975224915122\n",
            "[[34 68 43  9 50  4]\n",
            " [36 70 58 16 59  2]\n",
            " [31 83 63 23 64  1]\n",
            " [15 50 45 31 67  4]\n",
            " [23 42 52 23 98 11]\n",
            " [ 6  9 25  9 27 16]]\n"
          ]
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = rf.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'RandomForestClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPOGsDOn-lAI",
        "outputId": "469e5c6d-7211-4109-b8c1-c0246b0a9ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.23993685872138912\n",
            "Precision:  0.24102368113167535\n",
            "Recall:  0.23993685872138912\n",
            "F1:  0.2402004306580156\n",
            "[[49 52 43 21 37  6]\n",
            " [56 57 58 34 30  6]\n",
            " [42 62 63 51 34 13]\n",
            " [31 33 49 41 42 16]\n",
            " [38 37 40 44 74 16]\n",
            " [ 7 17 15 12 21 20]]\n"
          ]
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = svc.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_liar_speaker_party.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTkrjHse-lAI"
      },
      "outputs": [],
      "source": [
        "# binary liar using statement speaker and party\n",
        "\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['barely-true'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['barely-true'], 'false')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['pants-fire'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['pants-fire'], 'false')\n",
        "\n",
        "# vectorize the statement column\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(df_liar_train['statement'])\n",
        "tfidf_test = tfidf_vectorizer.transform(df_liar_test['statement'])\n",
        "\n",
        "# vectorize the speaker column\n",
        "tfidf_train_speaker = tfidf_vectorizer.fit_transform(df_liar_train['speaker'])\n",
        "tfidf_test_speaker = tfidf_vectorizer.transform(df_liar_test['speaker'])\n",
        "\n",
        "# vectorize the party column\n",
        "tfidf_train_party = tfidf_vectorizer.fit_transform(df_liar_train['party'])\n",
        "tfidf_test_party = tfidf_vectorizer.transform(df_liar_test['party'])\n",
        "\n",
        "# concatenate the three vectors\n",
        "tfidf_train = np.concatenate((tfidf_train.toarray(), tfidf_train_speaker.toarray(), tfidf_train_party.toarray()), axis=1)\n",
        "tfidf_test = np.concatenate((tfidf_test.toarray(), tfidf_test_speaker.toarray(), tfidf_test_party.toarray()), axis=1)\n",
        "\n",
        "label = ['true', 'false']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMXLanzV-lAI",
        "outputId": "97593ae2-a3cf-453f-84eb-2498803c74fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4262036306235201\n",
            "Precision:  0.43373585872217285\n",
            "Recall:  0.4262036306235201\n",
            "F1:  0.4295301446861111\n",
            "[[195 142]\n",
            " [161 280]]\n"
          ]
        }
      ],
      "source": [
        "# initialize a PassiveAggressiveClassifier\n",
        "\n",
        "pac = PassiveAggressiveClassifier(max_iter=1000)\n",
        "pac.fit(tfidf_train, df_liar_train['label'])\n",
        "\n",
        "# predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'PassiveAggressiveClassifier', label, first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPcOvFVE-lAI",
        "outputId": "7c081d3f-98fd-461c-b38a-740b8ba63d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.49171270718232046\n",
            "Precision:  0.4555053197588091\n",
            "Recall:  0.49171270718232046\n",
            "F1:  0.4641482017531922\n",
            "[[238 171]\n",
            " [155 361]]\n"
          ]
        }
      ],
      "source": [
        "# with logistic regression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = logreg.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LogisticRegression', label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMdzzmpb-lAJ",
        "outputId": "8b652996-dea8-43e3-c631-1bbea7690c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.5193370165745856\n",
            "Precision:  0.4799922721754087\n",
            "Recall:  0.5193370165745856\n",
            "F1:  0.46245516911100526\n",
            "[[249 196]\n",
            " [144 405]]\n"
          ]
        }
      ],
      "source": [
        "# with multinomial naive bayes\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = nb.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'MultinomialNB', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ugNeXrM-lAJ",
        "outputId": "555bc1d1-7e06-40cb-bbec-128d11398812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.48697711128650356\n",
            "Precision:  0.46545565923182264\n",
            "Recall:  0.48697711128650356\n",
            "F1:  0.47018379312967296\n",
            "[[188 187]\n",
            " [112 381]]\n"
          ]
        }
      ],
      "source": [
        "# with KNN\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = knn.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'KNeighborsClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v24_WrQs-lAJ",
        "outputId": "519ec06b-9ef0-428f-e596-ac03f9e470e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.5035516969218626\n",
            "Precision:  0.4672927839945014\n",
            "Recall:  0.5035516969218626\n",
            "F1:  0.44938154763518495\n",
            "[[236 207]\n",
            " [152 397]]\n"
          ]
        }
      ],
      "source": [
        "# with a random forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = rf.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'RandomForestClassifier', label=label, results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUZYXE8D-lAJ",
        "outputId": "aa58a814-3616-4cdc-988f-3ef424792211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.4585635359116022\n",
            "Precision:  0.4426506927816209\n",
            "Recall:  0.4585635359116022\n",
            "F1:  0.4486165963853383\n",
            "[[223 156]\n",
            " [165 316]]\n"
          ]
        }
      ],
      "source": [
        "# with a support vector machine\n",
        "\n",
        "svc = LinearSVC(random_state=0, dual='auto')\n",
        "svc.fit(tfidf_train, df_liar_train['label'])\n",
        "y_pred = svc.predict(tfidf_test)\n",
        "\n",
        "\n",
        "results = calculate_and_save_metrics(df_liar_test['label'], y_pred, 'LinearSVC', label=label, results=results, last=True, savename='results_liar_speaker_party_binary.csv')"
=======
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "\n",
        "                 statement_vocab_dim,\n",
        "                 subject_vocab_dim,\n",
        "                 speaker_vocab_dim,\n",
        "                 speaker_pos_vocab_dim,\n",
        "                 state_vocab_dim,\n",
        "                 party_vocab_dim,\n",
        "                 context_vocab_dim,\n",
        "\n",
        "                 statement_embed_dim = 100,\n",
        "                 statement_kernel_num = 14,\n",
        "                 statement_kernel_size = [3, 4, 5],\n",
        "\n",
        "                 subject_embed_dim = 5,\n",
        "                 subject_lstm_nlayers = 2,\n",
        "                 subject_lstm_bidirectional = True,\n",
        "                 subject_hidden_dim = 5,\n",
        "\n",
        "                 speaker_embed_dim = 5,\n",
        "\n",
        "                 speaker_pos_embed_dim = 10,\n",
        "                 speaker_pos_lstm_nlayers = 2,\n",
        "                 speaker_pos_lstm_bidirectional = True,\n",
        "                 speaker_pos_hidden_dim = 5,\n",
        "\n",
        "                 state_embed_dim = 5,\n",
        "\n",
        "                 party_embed_dim = 5,\n",
        "\n",
        "                 context_embed_dim = 20,\n",
        "                 context_lstm_nlayers = 2,\n",
        "                 context_lstm_bidirectional = True,\n",
        "                 context_hidden_dim = 6,\n",
        "                 dropout = 0.5):\n",
        "\n",
        "        # Statement CNN\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.statement_vocab_dim = statement_vocab_dim\n",
        "        self.statement_embed_dim = statement_embed_dim\n",
        "        self.statement_kernel_num = statement_kernel_num\n",
        "        self.statement_kernel_size = statement_kernel_size\n",
        "\n",
        "        self.statement_embedding = nn.Embedding(self.statement_vocab_dim, self.statement_embed_dim)\n",
        "        self.statement_convs = [nn.Conv2d(1, self.statement_kernel_num, (kernel_, self.statement_embed_dim)) for kernel_ in self.statement_kernel_size]\n",
        "\n",
        "        # Subject\n",
        "        self.subject_vocab_dim = subject_vocab_dim\n",
        "        self.subject_embed_dim = subject_embed_dim\n",
        "        self.subject_lstm_nlayers = subject_lstm_nlayers\n",
        "        self.subject_lstm_num_direction = 2 if subject_lstm_bidirectional else 1\n",
        "        self.subject_hidden_dim = subject_hidden_dim\n",
        "\n",
        "        self.subject_embedding = nn.Embedding(self.subject_vocab_dim, self.subject_embed_dim)\n",
        "        self.subject_lstm = nn.LSTM(\n",
        "            input_size = self.subject_embed_dim,\n",
        "            hidden_size = self.subject_hidden_dim,\n",
        "            num_layers = self.subject_lstm_nlayers,\n",
        "            batch_first = True,\n",
        "            bidirectional = subject_lstm_bidirectional\n",
        "        )\n",
        "\n",
        "        # Speaker\n",
        "        self.speaker_vocab_dim = speaker_vocab_dim\n",
        "        self.speaker_embed_dim = speaker_embed_dim\n",
        "\n",
        "        self.speaker_embedding = nn.Embedding(self.speaker_vocab_dim, self.speaker_embed_dim)\n",
        "\n",
        "        # Speaker Position\n",
        "        self.speaker_pos_vocab_dim = speaker_pos_vocab_dim\n",
        "        self.speaker_pos_embed_dim = speaker_pos_embed_dim\n",
        "        self.speaker_pos_lstm_nlayers = speaker_pos_lstm_nlayers\n",
        "        self.speaker_pos_lstm_num_direction = 2 if speaker_pos_lstm_bidirectional else 1\n",
        "        self.speaker_pos_hidden_dim = speaker_pos_hidden_dim\n",
        "\n",
        "        self.speaker_pos_embedding = nn.Embedding(self.speaker_pos_vocab_dim, self.speaker_pos_embed_dim)\n",
        "        self.speaker_pos_lstm = nn.LSTM(\n",
        "            input_size = self.speaker_pos_embed_dim,\n",
        "            hidden_size = self.speaker_pos_hidden_dim,\n",
        "            num_layers = self.speaker_pos_lstm_nlayers,\n",
        "            batch_first = True,\n",
        "            bidirectional = speaker_pos_lstm_bidirectional\n",
        "        )\n",
        "\n",
        "        # State\n",
        "        self.state_vocab_dim = state_vocab_dim\n",
        "        self.state_embed_dim = state_embed_dim\n",
        "\n",
        "        self.state_embedding = nn.Embedding(self.state_vocab_dim, self.state_embed_dim)\n",
        "\n",
        "        # Party\n",
        "        self.party_vocab_dim = party_vocab_dim\n",
        "        self.party_embed_dim = party_embed_dim\n",
        "\n",
        "        self.party_embedding = nn.Embedding(self.party_vocab_dim, self.party_embed_dim)\n",
        "\n",
        "        # Context\n",
        "        self.context_vocab_dim = context_vocab_dim\n",
        "        self.context_embed_dim = context_embed_dim\n",
        "        self.context_lstm_nlayers = context_lstm_nlayers\n",
        "        self.context_lstm_num_direction = 2 if context_lstm_bidirectional else 1\n",
        "        self.context_hidden_dim = context_hidden_dim\n",
        "\n",
        "        self.context_embedding = nn.Embedding(self.context_vocab_dim, self.context_embed_dim)\n",
        "        self.context_lstm = nn.LSTM(\n",
        "            input_size = self.context_embed_dim,\n",
        "            hidden_size = self.context_hidden_dim,\n",
        "            num_layers = self.context_lstm_nlayers,\n",
        "            batch_first = True,\n",
        "            bidirectional = context_lstm_bidirectional\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(len(self.statement_kernel_size) * self.statement_kernel_num\n",
        "                            + self.subject_lstm_nlayers * self.subject_lstm_num_direction\n",
        "                            + self.speaker_embed_dim\n",
        "                            + self.speaker_pos_lstm_nlayers * self.speaker_pos_lstm_num_direction\n",
        "                            + self.state_embed_dim\n",
        "                            + self.party_embed_dim\n",
        "                            + self.context_lstm_nlayers * self.context_lstm_num_direction,\n",
        "                            6)\n",
        "\n",
        "    def forward(self,\n",
        "                sample):\n",
        "        statement = Variable(sample.statement).unsqueeze(0)\n",
        "        subject = Variable(sample.subject).unsqueeze(0)\n",
        "        speaker = Variable(sample.speaker).unsqueeze(0)\n",
        "        speaker_pos = Variable(sample.speaker_pos).unsqueeze(0)\n",
        "        state = Variable(sample.state).unsqueeze(0)\n",
        "        party = Variable(sample.party).unsqueeze(0)\n",
        "        context = Variable(sample.context).unsqueeze(0)\n",
        "\n",
        "        batch = 1 # Current support one sample per time\n",
        "                  # TODO: Increase batch number\n",
        "\n",
        "        # Statement\n",
        "        statement_ = self.statement_embedding(statement).unsqueeze(0) # 1*W*D -> 1*1*W*D\n",
        "        statement_ = [F.relu(conv(statement_)).squeeze(3) for conv in self.statement_convs] # 1*1*W*1 -> 1*Co*W x [len(convs)]\n",
        "        statement_ = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in statement_] # 1*Co*1 -> 1*Co x len(convs)\n",
        "        statement_ = torch.cat(statement_, 1)  # 1*len(convs)\n",
        "\n",
        "        # Subject\n",
        "        subject_ = self.subject_embedding(subject) # 1*W*D\n",
        "        _, (subject_, _) = self.subject_lstm(subject_) # 1*(layer x dir)*hidden\n",
        "        subject_ = F.max_pool1d(subject_, self.subject_hidden_dim).view(1, -1) # 1*(layer x dir)*1 -> 1*(layer x dir)\n",
        "\n",
        "        # Speaker\n",
        "        speaker_ = self.speaker_embedding(speaker).squeeze(0) # 1*1*D -> 1*D\n",
        "\n",
        "        # Speaker Position\n",
        "        speaker_pos_ = self.speaker_pos_embedding(speaker_pos)\n",
        "        _, (speaker_pos_, _) = self.speaker_pos_lstm(speaker_pos_)\n",
        "        speaker_pos_ = F.max_pool1d(speaker_pos_, self.speaker_pos_hidden_dim).view(1, -1)\n",
        "\n",
        "        # State\n",
        "        state_ = self.state_embedding(state).squeeze(0)\n",
        "\n",
        "        # Party\n",
        "        party_ = self.party_embedding(party).squeeze(0)\n",
        "\n",
        "        # Context\n",
        "        context_ = self.context_embedding(context)\n",
        "        _, (context_, _) = self.context_lstm(context_)\n",
        "        context_ = F.max_pool1d(context_, self.context_hidden_dim).view(1, -1)\n",
        "\n",
        "        # Concatenate\n",
        "        features = torch.cat((statement_, subject_, speaker_, speaker_pos_, state_, party_, context_), 1)\n",
        "        features = self.dropout(features)\n",
        "        features = self.fc(features)\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "num_to_label = ['pants-fire',\n",
        "                'false',\n",
        "                'barely-true',\n",
        "                'half-true',\n",
        "                'mostly-true',\n",
        "                'true']\n",
        "\n",
        "label_to_number = {\n",
        "\t'pants-fire': 0,\n",
        "\t'false': 1,\n",
        "\t'barely-true': 2,\n",
        "\t'half-true': 3,\n",
        "\t'mostly-true': 4,\n",
        "\t'true': 5\n",
        "}\n",
        "\n",
        "def valid(valid_samples, word2num, model):\n",
        "    acc = 0\n",
        "    for sample in valid_samples:\n",
        "        prediction = model(sample)\n",
        "        prediction = int(np.argmax(prediction.data.numpy()))\n",
        "        if prediction == sample.label:\n",
        "            acc += 1\n",
        "    acc /= len(valid_samples)\n",
        "    print('  Validation Accuracy: '+str(acc))\n",
        "\n",
        "def find_word(word2num, token):\n",
        "    if token in word2num:\n",
        "        return word2num[token]\n",
        "    else:\n",
        "        return word2num['<unk>']\n",
        "    \n",
        "def test_data_prepare(test_file, word2num, phase):\n",
        "    test_input = open(test_file, 'rb')\n",
        "    test_data = test_input.read().decode('utf-8')\n",
        "    test_input.close()\n",
        "\n",
        "    statement_word2num = word2num[0]\n",
        "    subject_word2num = word2num[1]\n",
        "    speaker_word2num = word2num[2]\n",
        "    speaker_pos_word2num = word2num[3]\n",
        "    state_word2num = word2num[4]\n",
        "    party_word2num = word2num[5]\n",
        "    context_word2num = word2num[6]\n",
        "\n",
        "    test_samples = []\n",
        "\n",
        "    for line in test_data.strip().split('\\n'):\n",
        "        tmp = line.strip().split('\\t')\n",
        "        while len(tmp) != 8:\n",
        "            tmp.append('')\n",
        "        if phase == 'test':\n",
        "            p = DataSample('test', tmp[0], tmp[1], tmp[2], tmp[3], tmp[4], tmp[5], tmp[6])\n",
        "        elif phase == 'valid':\n",
        "            p = DataSample(tmp[0], tmp[1], tmp[2], tmp[3], tmp[4], tmp[5], tmp[6], tmp[7])\n",
        "\n",
        "        for i in range(len(p.statement)):\n",
        "            p.statement[i] = find_word(statement_word2num, p.statement[i])\n",
        "        for i in range(len(p.subject)):\n",
        "            p.subject[i] = find_word(subject_word2num, p.subject[i])\n",
        "        p.speaker = find_word(speaker_word2num, p.speaker)\n",
        "        for i in range(len(p.speaker_pos)):\n",
        "            p.speaker_pos[i] = find_word(speaker_pos_word2num, p.speaker_pos[i])\n",
        "        p.state = find_word(state_word2num, p.state)\n",
        "        p.party = find_word(party_word2num, p.party)\n",
        "        for i in range(len(p.context)):\n",
        "            p.context[i] = find_word(context_word2num, p.context[i])\n",
        "\n",
        "        test_samples.append(p)\n",
        "\n",
        "    return test_samples\n",
        "\n",
        "def test(test_file, test_output, word2num, model, use_cuda = False):\n",
        "    test_samples = test_data_prepare(test_file, word2num, 'test')\n",
        "    dataset_to_variable(test_samples, use_cuda)\n",
        "    out = open(test_output, 'w')\n",
        "\n",
        "    for sample in test_samples:\n",
        "        prediction = model(sample)\n",
        "        prediction = int(np.argmax(prediction.data.numpy()))\n",
        "        out.write(num_to_label[prediction]+'\\n')\n",
        "\n",
        "    out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_to_variable(dataset, use_cuda):\n",
        "\tfor i in range(len(dataset)):\n",
        "\t\tdataset[i].statement = torch.LongTensor(dataset[i].statement)\n",
        "\t\tdataset[i].subject = torch.LongTensor(dataset[i].subject)\n",
        "\t\tdataset[i].speaker = torch.LongTensor([dataset[i].speaker])\n",
        "\t\tdataset[i].speaker_pos = torch.LongTensor(dataset[i].speaker_pos)\n",
        "\t\tdataset[i].state = torch.LongTensor([dataset[i].state])\n",
        "\t\tdataset[i].party = torch.LongTensor([dataset[i].party])\n",
        "\t\tdataset[i].context = torch.LongTensor(dataset[i].context)\n",
        "\t\tif use_cuda:\n",
        "\t\t\tdataset[i].statement.cuda()\n",
        "\t\t\tdataset[i].subject.cuda()\n",
        "\t\t\tdataset[i].speaker.cuda()\n",
        "\t\t\tdataset[i].speaker_pos.cuda()\n",
        "\t\t\tdataset[i].state.cuda()\n",
        "\t\t\tdataset[i].party.cuda()\n",
        "\t\t\tdataset[i].context.cuda()\n",
        "\n",
        "def train(train_samples,\n",
        "          valid_samples,\n",
        "          word2num,\n",
        "          lr = 0.001,\n",
        "          epoch = 5,\n",
        "          use_cuda = False):\n",
        "\n",
        "    print('Training...')\n",
        "\n",
        "    # Prepare training data\n",
        "    print('  Preparing training data...')\n",
        "    statement_word2num = word2num[0]\n",
        "    subject_word2num = word2num[1]\n",
        "    speaker_word2num = word2num[2]\n",
        "    speaker_pos_word2num = word2num[3]\n",
        "    state_word2num = word2num[4]\n",
        "    party_word2num = word2num[5]\n",
        "    context_word2num = word2num[6]\n",
        "\n",
        "    train_data = train_samples\n",
        "    dataset_to_variable(train_data, use_cuda)\n",
        "    valid_data = valid_samples\n",
        "    dataset_to_variable(valid_data, use_cuda)\n",
        "\n",
        "    # Construct model instance\n",
        "    print('  Constructing network model...')\n",
        "    model = Net(len(statement_word2num),\n",
        "                len(subject_word2num),\n",
        "                len(speaker_word2num),\n",
        "                len(speaker_pos_word2num),\n",
        "                len(state_word2num),\n",
        "                len(party_word2num),\n",
        "                len(context_word2num))\n",
        "    if use_cuda: model.cuda()\n",
        "\n",
        "    # Start training\n",
        "    print('  Start training')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    model.train()\n",
        "\n",
        "    step = 0\n",
        "    display_interval = 2000\n",
        "\n",
        "    for epoch_ in range(epoch):\n",
        "        print('  ==> Epoch '+str(epoch_)+' started.')\n",
        "        random.shuffle(train_data)\n",
        "        total_loss = 0\n",
        "        for sample in train_data:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            prediction = model(sample)\n",
        "            label = Variable(torch.LongTensor([sample.label]))\n",
        "            loss = F.cross_entropy(prediction, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "            if step % display_interval == 0:\n",
        "                print('    ==> Iter: '+str(step)+' Loss: '+str(loss))\n",
        "\n",
        "            total_loss += loss.data.numpy()\n",
        "\n",
        "        print('  ==> Epoch '+str(epoch_)+' finished. Avg Loss: '+str(total_loss/len(train_data)))\n",
        "\n",
        "        valid(valid_data, word2num, model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using Bert\n",
        "\n",
        "\n",
        "# set random seeds to ensure reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# set batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define fields\n",
        "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# make splits for data liar dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V3AviGL9-It_"
      },
      "outputs": [],
      "source": [
        "# using deep learning torch\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = pd.read_csv(csv_file, on_bad_lines='skip', sep='\\t', header=None)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        text = self.data.iloc[idx, 2]\n",
        "        label = self.data.iloc[idx, 1]\n",
        "        #print(text, label)\n",
        "\n",
        "        if self.transform:\n",
        "            text = self.transform(text)\n",
        "\n",
        "        return text, label\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return torch.from_numpy(text).float()\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1,\n",
        "                                              out_channels = n_filters,\n",
        "                                              kernel_size = (fs, embedding_dim))\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        #text = text.permute(1, 0)\n",
        "\n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        #transform text to tensor\n",
        "\n",
        "        text = text.unsqueeze(1)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        #pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for text, labels in iterator:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predictions = model(text).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for text, labels in iterator:\n",
        "\n",
        "            predictions = model(text).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
<<<<<<< HEAD
        "id": "WSO7yXfNG0OU",
        "outputId": "7b52a9e2-c885-4fd0-a644-3abb18c891df"
=======
        "id": "Q2LxY8yKAvMy",
        "outputId": "47dac185-7cb9-42a6-efc3-b43ceb25413b"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
=======
            "Words not found in GloVe embeddings: 581\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
          ]
        }
      ],
      "source": [
<<<<<<< HEAD
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP9wU23BHWCf",
        "outputId": "ec96c875-e95f-4637-e175-9e429d1df765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting spacy==2.3.5\n",
            "  Using cached spacy-2.3.5.tar.gz (5.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (3.0.9)\n",
            "Collecting thinc<7.5.0,>=7.4.1 (from spacy==2.3.5)\n",
            "  Using cached thinc-7.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.5)\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.5)\n",
            "  Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.5)\n",
            "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (4.66.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (69.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.23.5)\n",
            "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.3.5)\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2023.11.17)\n",
            "Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (369 kB)\n",
            "Building wheels for collected packages: spacy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for spacy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for spacy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build spacy\n",
            "\u001b[31mERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m2024-01-11 15:39:17.995010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-11 15:39:17.995065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-11 15:39:17.996381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-11 15:39:18.003491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-11 15:39:19.085837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-11 15:39:20.565373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-11 15:39:20.565783: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-11 15:39:20.565938: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "!pip install -U spacy==2.3.5\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eujZv5vMI8k4",
        "outputId": "6ceba7ae-5f0b-4c09-f064-51d683176d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.3.5\n",
            "  Using cached spacy-2.3.5.tar.gz (5.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (3.0.9)\n",
            "Collecting thinc<7.5.0,>=7.4.1 (from spacy==2.3.5)\n",
            "  Using cached thinc-7.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.5)\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.5)\n",
            "  Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.5)\n",
            "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (4.66.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (69.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.23.5)\n",
            "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.3.5)\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2023.11.17)\n",
            "Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (369 kB)\n",
            "Building wheels for collected packages: spacy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for spacy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for spacy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build spacy\n",
            "\u001b[31mERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install spacy==2.3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VYUIsir0L-72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "import datetime\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eTEKiMuvE7-W"
      },
      "outputs": [],
      "source": [
        "# use bert to classify the news using pytorch\n",
        "\n",
        "# set the seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize_spacy(text):\n",
        "    return [token.text for token in nlp(text)]\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize_spacy, include_lengths=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xC4AuyhZMlvc"
      },
      "outputs": [],
      "source": [
        "# load the liar dataset\n",
        "csv_path_liar_train = os.path.join( '/content', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( '/content', 'liar_dataset', 'test.tsv')\n",
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "\n",
        "df_liar_train = df_liar_train[['label', 'statement']]\n",
        "df_liar_test = df_liar_test[['label', 'statement']]\n",
        "df_liar_train = df_liar_train.dropna()\n",
        "df_liar_test = df_liar_test.dropna()\n",
        "\n",
        "# split the data into train and test sets\n",
        "train, test = train_test_split(df_liar_train, test_size=0.1, random_state=7)\n",
        "\n",
        "# save the train and test sets to csv files\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v3aLPkoHMov4"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='train.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=[('label', LABEL), ('text', TEXT)]\n",
        ")\n",
        "\n",
        "# split the train data into train and validation sets\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "# build the vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data,\n",
        "                    max_size=MAX_VOCAB_SIZE,\n",
        "                    vectors=\"glove.6B.100d\",\n",
        "                    unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f5scfrTfPt9M"
      },
      "outputs": [],
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QhMDTRaCNxxy"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "\n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fm8C0JQlN2AI"
      },
      "outputs": [],
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 6\n",
        "N_LAYERS = 4\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.7\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = RNN(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
=======
        "# Set hyperparameters\n",
        "size_of_vocab = len(tfidf_vectorizer.vocabulary_) + 1  # Add 1 for the <UNK> token\n",
        "embedding_dim = 100\n",
        "n_filters = 100\n",
        "filter_sizes = [3, 4, 5]\n",
        "output_dim = 1\n",
        "dropout = 0.5\n",
        "batch_size = 64\n",
        "\n",
        "# Instantiate the model\n",
        "model = TextCNN(size_of_vocab, embedding_dim, n_filters, filter_sizes, output_dim, dropout)\n",
        "model = model.to(device)\n",
        "\n",
        "glove = vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Get the embeddings corresponding to words in both GloVe and your TF-IDF vocabulary\n",
        "embedding_vectors = []\n",
        "words_not_found = 0\n",
        "unk_token = torch.zeros(embedding_dim)  # Placeholder for <UNK> token\n",
        "\n",
        "for word in tfidf_vectorizer.vocabulary_:\n",
        "    if word in glove.stoi:\n",
        "        embedding_vectors.append(glove[word])\n",
        "    else:\n",
        "        # For words not found in GloVe, use the <UNK> token\n",
        "        embedding_vectors.append(unk_token)\n",
        "        words_not_found += 1\n",
        "\n",
        "pretrained_embeddings = torch.stack(embedding_vectors)\n",
        "\n",
        "# Add the <UNK> token vector to pretrained_embeddings\n",
        "pretrained_embeddings = torch.cat((pretrained_embeddings, unk_token.unsqueeze(0)), dim=0)\n",
        "\n",
        "# Print the count of words not found in GloVe\n",
        "print(f\"Words not found in GloVe embeddings: {words_not_found}\")\n",
        "\n",
        "# Ensure the dimensions match\n",
        "if pretrained_embeddings.size(0) != size_of_vocab:\n",
        "    raise ValueError(\"Dimensions of embeddings do not match the size of vocabulary.\")\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "# Initialize the optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#split the data into train and test sets using test.tsv and train.tsv\n",
        "train_dataset = NewsDataset(csv_file='/content/liar_dataset/train.tsv')\n",
        "test_dataset = NewsDataset(csv_file='/content/liar_dataset/test.tsv')\n",
        "\n",
        "#split the train dataset into train and validation sets\n",
        "train_data, valid_data = torch.utils.data.random_split(train_dataset, [len(train_dataset)-1000, 1000])\n",
        "\n",
        "#initialize the dataloaders\n",
        "train_iterator = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=batch_size)\n"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 9,
      "metadata": {
        "id": "c1WGVGU-N6OB"
      },
      "outputs": [],
      "source": [
        "# define the accuracy function\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    return correct.float() / y.shape[0]\n",
        "\n",
        "# define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the evaluation function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(iterator):\n",
        "\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "                acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the function to calculate the time elapsed\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnDftL05OCH8",
        "outputId": "ae2ce0e6-1354-47e1-fb05-e0a867223497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 35.62it/s]\n",
            "100%|| 44/44 [00:00<00:00, 101.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.775 | Train Acc: 18.75%\n",
            "\t Val. Loss: 1.753 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 37.39it/s]\n",
            "100%|| 44/44 [00:00<00:00, 73.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.763 | Train Acc: 19.23%\n",
            "\t Val. Loss: 1.750 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 35.30it/s]\n",
            "100%|| 44/44 [00:00<00:00, 53.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.762 | Train Acc: 20.39%\n",
            "\t Val. Loss: 1.751 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:03<00:00, 27.54it/s]\n",
            "100%|| 44/44 [00:00<00:00, 120.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 1.762 | Train Acc: 20.03%\n",
            "\t Val. Loss: 1.749 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 46.11it/s]\n",
            "100%|| 44/44 [00:00<00:00, 121.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.762 | Train Acc: 18.72%\n",
            "\t Val. Loss: 1.750 |  Val. Acc: 21.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 46.16it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.759 | Train Acc: 20.45%\n",
            "\t Val. Loss: 1.748 |  Val. Acc: 22.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 46.10it/s]\n",
            "100%|| 44/44 [00:00<00:00, 121.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.754 | Train Acc: 20.67%\n",
            "\t Val. Loss: 1.743 |  Val. Acc: 23.03%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.07it/s]\n",
            "100%|| 44/44 [00:00<00:00, 105.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.751 | Train Acc: 21.66%\n",
            "\t Val. Loss: 1.741 |  Val. Acc: 22.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.67it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.749 | Train Acc: 21.20%\n",
            "\t Val. Loss: 1.745 |  Val. Acc: 22.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.53it/s]\n",
            "100%|| 44/44 [00:00<00:00, 116.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.748 | Train Acc: 22.14%\n",
            "\t Val. Loss: 1.740 |  Val. Acc: 22.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.40it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.746 | Train Acc: 21.78%\n",
            "\t Val. Loss: 1.754 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.26it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.746 | Train Acc: 22.81%\n",
            "\t Val. Loss: 1.750 |  Val. Acc: 21.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.87it/s]\n",
            "100%|| 44/44 [00:00<00:00, 108.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.744 | Train Acc: 21.62%\n",
            "\t Val. Loss: 1.735 |  Val. Acc: 23.81%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.96it/s]\n",
            "100%|| 44/44 [00:00<00:00, 116.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.736 | Train Acc: 23.38%\n",
            "\t Val. Loss: 1.741 |  Val. Acc: 22.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.55it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.740 | Train Acc: 22.49%\n",
            "\t Val. Loss: 1.737 |  Val. Acc: 22.47%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.71it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.734 | Train Acc: 23.33%\n",
            "\t Val. Loss: 1.743 |  Val. Acc: 22.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.72it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.737 | Train Acc: 22.03%\n",
            "\t Val. Loss: 1.739 |  Val. Acc: 23.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.13it/s]\n",
            "100%|| 44/44 [00:00<00:00, 111.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.734 | Train Acc: 23.05%\n",
            "\t Val. Loss: 1.737 |  Val. Acc: 22.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.93it/s]\n",
            "100%|| 44/44 [00:00<00:00, 111.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.735 | Train Acc: 22.63%\n",
            "\t Val. Loss: 1.755 |  Val. Acc: 23.15%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 38.68it/s]\n",
            "100%|| 44/44 [00:00<00:00, 114.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.730 | Train Acc: 23.04%\n",
            "\t Val. Loss: 1.746 |  Val. Acc: 22.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.39it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.723 | Train Acc: 23.07%\n",
            "\t Val. Loss: 1.742 |  Val. Acc: 22.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.76it/s]\n",
            "100%|| 44/44 [00:00<00:00, 104.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.724 | Train Acc: 23.18%\n",
            "\t Val. Loss: 1.741 |  Val. Acc: 23.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.88it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.722 | Train Acc: 23.85%\n",
            "\t Val. Loss: 1.747 |  Val. Acc: 21.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.71it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.722 | Train Acc: 24.21%\n",
            "\t Val. Loss: 1.749 |  Val. Acc: 22.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.78it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.721 | Train Acc: 23.90%\n",
            "\t Val. Loss: 1.745 |  Val. Acc: 21.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.05it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.713 | Train Acc: 25.01%\n",
            "\t Val. Loss: 1.747 |  Val. Acc: 22.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.79it/s]\n",
            "100%|| 44/44 [00:00<00:00, 108.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.714 | Train Acc: 25.21%\n",
            "\t Val. Loss: 1.754 |  Val. Acc: 22.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.22it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.712 | Train Acc: 24.91%\n",
            "\t Val. Loss: 1.767 |  Val. Acc: 22.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.94it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.710 | Train Acc: 24.78%\n",
            "\t Val. Loss: 1.775 |  Val. Acc: 21.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.96it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.704 | Train Acc: 25.15%\n",
            "\t Val. Loss: 1.766 |  Val. Acc: 22.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.76it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.697 | Train Acc: 25.72%\n",
            "\t Val. Loss: 1.792 |  Val. Acc: 21.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.40it/s]\n",
            "100%|| 44/44 [00:00<00:00, 106.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.697 | Train Acc: 25.94%\n",
            "\t Val. Loss: 1.803 |  Val. Acc: 21.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.79it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.697 | Train Acc: 25.09%\n",
            "\t Val. Loss: 1.747 |  Val. Acc: 23.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.99it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.698 | Train Acc: 25.38%\n",
            "\t Val. Loss: 1.753 |  Val. Acc: 22.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.01it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.688 | Train Acc: 26.26%\n",
            "\t Val. Loss: 1.760 |  Val. Acc: 22.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.16it/s]\n",
            "100%|| 44/44 [00:00<00:00, 114.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.690 | Train Acc: 26.49%\n",
            "\t Val. Loss: 1.778 |  Val. Acc: 22.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.67it/s]\n",
            "100%|| 44/44 [00:00<00:00, 110.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.683 | Train Acc: 26.39%\n",
            "\t Val. Loss: 1.766 |  Val. Acc: 22.15%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.10it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.677 | Train Acc: 27.24%\n",
            "\t Val. Loss: 1.838 |  Val. Acc: 21.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.50it/s]\n",
            "100%|| 44/44 [00:00<00:00, 120.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.686 | Train Acc: 26.61%\n",
            "\t Val. Loss: 1.794 |  Val. Acc: 23.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.32it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.682 | Train Acc: 27.05%\n",
            "\t Val. Loss: 1.785 |  Val. Acc: 22.83%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.54it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.675 | Train Acc: 26.72%\n",
            "\t Val. Loss: 1.788 |  Val. Acc: 23.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.77it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.680 | Train Acc: 27.37%\n",
            "\t Val. Loss: 1.805 |  Val. Acc: 22.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 41.56it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.667 | Train Acc: 28.32%\n",
            "\t Val. Loss: 1.797 |  Val. Acc: 22.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.25it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.667 | Train Acc: 27.81%\n",
            "\t Val. Loss: 1.882 |  Val. Acc: 21.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.20it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.665 | Train Acc: 27.77%\n",
            "\t Val. Loss: 1.795 |  Val. Acc: 23.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.86it/s]\n",
            "100%|| 44/44 [00:00<00:00, 107.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.665 | Train Acc: 27.95%\n",
            "\t Val. Loss: 1.812 |  Val. Acc: 23.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.66it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.669 | Train Acc: 27.61%\n",
            "\t Val. Loss: 1.783 |  Val. Acc: 22.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.07it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.656 | Train Acc: 28.24%\n",
            "\t Val. Loss: 1.816 |  Val. Acc: 22.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.07it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.658 | Train Acc: 28.52%\n",
            "\t Val. Loss: 1.827 |  Val. Acc: 22.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 38.31it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.654 | Train Acc: 28.88%\n",
            "\t Val. Loss: 1.875 |  Val. Acc: 21.79%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.63it/s]\n",
            "100%|| 44/44 [00:00<00:00, 105.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 51 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.652 | Train Acc: 29.12%\n",
            "\t Val. Loss: 1.860 |  Val. Acc: 21.69%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.63it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 52 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.652 | Train Acc: 28.96%\n",
            "\t Val. Loss: 1.823 |  Val. Acc: 21.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.01it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 53 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.650 | Train Acc: 28.66%\n",
            "\t Val. Loss: 1.831 |  Val. Acc: 21.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.20it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 54 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.643 | Train Acc: 29.22%\n",
            "\t Val. Loss: 1.857 |  Val. Acc: 20.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.99it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 55 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.646 | Train Acc: 28.84%\n",
            "\t Val. Loss: 1.869 |  Val. Acc: 21.27%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.82it/s]\n",
            "100%|| 44/44 [00:00<00:00, 106.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 56 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.644 | Train Acc: 29.79%\n",
            "\t Val. Loss: 1.837 |  Val. Acc: 22.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.85it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 57 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.643 | Train Acc: 28.90%\n",
            "\t Val. Loss: 1.828 |  Val. Acc: 22.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.84it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 58 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.642 | Train Acc: 29.52%\n",
            "\t Val. Loss: 1.805 |  Val. Acc: 23.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.27it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 59 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.634 | Train Acc: 30.11%\n",
            "\t Val. Loss: 1.880 |  Val. Acc: 22.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 40.09it/s]\n",
            "100%|| 44/44 [00:00<00:00, 104.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 60 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.629 | Train Acc: 30.43%\n",
            "\t Val. Loss: 1.899 |  Val. Acc: 22.22%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 41.96it/s]\n",
            "100%|| 44/44 [00:00<00:00, 116.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 61 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.634 | Train Acc: 29.93%\n",
            "\t Val. Loss: 1.876 |  Val. Acc: 21.34%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.04it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 62 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.630 | Train Acc: 30.35%\n",
            "\t Val. Loss: 1.891 |  Val. Acc: 20.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.53it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 63 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.629 | Train Acc: 30.94%\n",
            "\t Val. Loss: 1.892 |  Val. Acc: 20.80%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.26it/s]\n",
            "100%|| 44/44 [00:00<00:00, 116.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 64 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.632 | Train Acc: 30.64%\n",
            "\t Val. Loss: 1.932 |  Val. Acc: 21.47%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.56it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 65 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.620 | Train Acc: 30.57%\n",
            "\t Val. Loss: 1.917 |  Val. Acc: 21.27%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 37.08it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 66 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.620 | Train Acc: 31.50%\n",
            "\t Val. Loss: 1.909 |  Val. Acc: 20.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.46it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 67 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.621 | Train Acc: 30.21%\n",
            "\t Val. Loss: 1.903 |  Val. Acc: 21.34%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.23it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 68 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.617 | Train Acc: 31.01%\n",
            "\t Val. Loss: 1.894 |  Val. Acc: 21.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.37it/s]\n",
            "100%|| 44/44 [00:00<00:00, 120.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 69 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.610 | Train Acc: 31.54%\n",
            "\t Val. Loss: 1.864 |  Val. Acc: 22.22%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.82it/s]\n",
            "100%|| 44/44 [00:00<00:00, 106.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 70 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.616 | Train Acc: 31.67%\n",
            "\t Val. Loss: 1.945 |  Val. Acc: 22.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.66it/s]\n",
            "100%|| 44/44 [00:00<00:00, 120.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 71 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.608 | Train Acc: 31.46%\n",
            "\t Val. Loss: 1.919 |  Val. Acc: 21.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.00it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 72 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.592 | Train Acc: 32.04%\n",
            "\t Val. Loss: 1.913 |  Val. Acc: 21.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.15it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 73 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.603 | Train Acc: 31.79%\n",
            "\t Val. Loss: 1.936 |  Val. Acc: 22.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.99it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 74 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.607 | Train Acc: 31.53%\n",
            "\t Val. Loss: 1.943 |  Val. Acc: 22.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.14it/s]\n",
            "100%|| 44/44 [00:00<00:00, 105.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 75 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.597 | Train Acc: 32.48%\n",
            "\t Val. Loss: 1.913 |  Val. Acc: 22.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.51it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 76 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.590 | Train Acc: 32.89%\n",
            "\t Val. Loss: 1.991 |  Val. Acc: 21.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.97it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 77 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.594 | Train Acc: 32.62%\n",
            "\t Val. Loss: 1.909 |  Val. Acc: 22.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.45it/s]\n",
            "100%|| 44/44 [00:00<00:00, 120.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 78 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.598 | Train Acc: 32.26%\n",
            "\t Val. Loss: 1.930 |  Val. Acc: 21.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.41it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 79 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.583 | Train Acc: 32.77%\n",
            "\t Val. Loss: 1.958 |  Val. Acc: 21.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.64it/s]\n",
            "100%|| 44/44 [00:00<00:00, 106.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 80 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.590 | Train Acc: 33.10%\n",
            "\t Val. Loss: 1.967 |  Val. Acc: 21.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.82it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 81 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.583 | Train Acc: 32.36%\n",
            "\t Val. Loss: 1.917 |  Val. Acc: 22.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.53it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 82 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.595 | Train Acc: 32.39%\n",
            "\t Val. Loss: 1.948 |  Val. Acc: 21.51%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.42it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 83 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.588 | Train Acc: 33.58%\n",
            "\t Val. Loss: 1.939 |  Val. Acc: 21.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.35it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 84 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.586 | Train Acc: 32.21%\n",
            "\t Val. Loss: 1.945 |  Val. Acc: 22.09%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 42.74it/s]\n",
            "100%|| 44/44 [00:00<00:00, 112.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 85 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.572 | Train Acc: 34.57%\n",
            "\t Val. Loss: 1.960 |  Val. Acc: 23.11%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.24it/s]\n",
            "100%|| 44/44 [00:00<00:00, 116.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 86 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.577 | Train Acc: 33.18%\n",
            "\t Val. Loss: 2.044 |  Val. Acc: 22.28%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.12it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 87 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.576 | Train Acc: 34.47%\n",
            "\t Val. Loss: 2.024 |  Val. Acc: 21.98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.15it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 88 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.571 | Train Acc: 33.87%\n",
            "\t Val. Loss: 1.979 |  Val. Acc: 21.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.81it/s]\n",
            "100%|| 44/44 [00:00<00:00, 105.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 89 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.570 | Train Acc: 33.89%\n",
            "\t Val. Loss: 2.006 |  Val. Acc: 21.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.34it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 90 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.555 | Train Acc: 34.06%\n",
            "\t Val. Loss: 1.985 |  Val. Acc: 21.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.36it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 91 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.566 | Train Acc: 34.14%\n",
            "\t Val. Loss: 1.969 |  Val. Acc: 21.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.92it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 92 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.573 | Train Acc: 33.73%\n",
            "\t Val. Loss: 1.938 |  Val. Acc: 23.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.00it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 93 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.554 | Train Acc: 35.06%\n",
            "\t Val. Loss: 1.979 |  Val. Acc: 23.60%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.63it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 94 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.550 | Train Acc: 34.77%\n",
            "\t Val. Loss: 1.986 |  Val. Acc: 22.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.01it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 95 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.551 | Train Acc: 34.91%\n",
            "\t Val. Loss: 2.007 |  Val. Acc: 22.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.22it/s]\n",
            "100%|| 44/44 [00:00<00:00, 115.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 96 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.551 | Train Acc: 35.48%\n",
            "\t Val. Loss: 2.016 |  Val. Acc: 22.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.07it/s]\n",
            "100%|| 44/44 [00:00<00:00, 118.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 97 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.552 | Train Acc: 35.01%\n",
            "\t Val. Loss: 2.043 |  Val. Acc: 21.83%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 45.11it/s]\n",
            "100%|| 44/44 [00:00<00:00, 117.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 98 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.548 | Train Acc: 35.55%\n",
            "\t Val. Loss: 1.972 |  Val. Acc: 22.19%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 43.21it/s]\n",
            "100%|| 44/44 [00:00<00:00, 109.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.543 | Train Acc: 35.04%\n",
            "\t Val. Loss: 2.027 |  Val. Acc: 23.46%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:02<00:00, 44.00it/s]\n",
            "100%|| 44/44 [00:00<00:00, 119.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.556 | Train Acc: 34.32%\n",
            "\t Val. Loss: 2.009 |  Val. Acc: 23.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
=======
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L3qH3LBvIog9",
        "outputId": "04db4d99-d57a-4668-8e63-6bee4f57848b"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-875eebba5bcc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fbfc98382caf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fbfc98382caf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#text = [batch size, sent len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#embedded = [batch size, sent len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not tuple"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
          ]
        }
      ],
      "source": [
<<<<<<< HEAD
        "# train the model\n",
        "\n",
        "N_EPOCHS = 100\n",
=======
        "#train the model\n",
        "N_EPOCHS = 5\n",
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
<<<<<<< HEAD
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'liar-model.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Hn8NSo-lAJ",
        "outputId": "58b8b961-b481-4863-d3be-66ce387cd919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 16/16 [00:00<00:00, 65.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.738 | Test Acc: 21.19%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model on the test set\n",
        "\n",
        "model.load_state_dict(torch.load('liar-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
=======
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "#test the model\n",
        "model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
>>>>>>> 311832406d0ca01bbe2526a70f60c5c0e12d877b
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
