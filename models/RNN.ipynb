{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown==4.6.0\n",
        "!gdown --folder https://drive.google.com/drive/u/1/folders/15Wn46r7gidaiZbx2ArFYsd7rjYH4y7JM\n",
        "\n",
        "!pip install torchtext==0.6.0\n",
        "\n",
        "!pip install -U pip setuptools wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VuFc31HvWe6T",
        "outputId": "9e0d38f9-f6f7-4176-ef7f-1d95b6996b2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.6.0\n",
            "Retrieving folder list\n",
            "Processing file 1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C README\n",
            "Processing file 1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG test.tsv\n",
            "Processing file 1RvXY274ln1cKZR6LaUacyytgrrxNqH3K train.tsv\n",
            "Processing file 1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi valid.tsv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C\n",
            "To: /content/liar_dataset/README\n",
            "100% 1.67k/1.67k [00:00<00:00, 6.20MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG\n",
            "To: /content/liar_dataset/test.tsv\n",
            "100% 301k/301k [00:00<00:00, 3.53MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RvXY274ln1cKZR6LaUacyytgrrxNqH3K\n",
            "To: /content/liar_dataset/train.tsv\n",
            "100% 2.41M/2.41M [00:00<00:00, 15.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi\n",
            "To: /content/liar_dataset/valid.tsv\n",
            "100% 302k/302k [00:00<00:00, 3.59MB/s]\n",
            "Download completed\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.3.2 setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PsOSJ6MZUYtZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import datetime\n",
        "import spacy\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
        "def tokenize_spacy(text):\n",
        "    return [token.text for token in nlp(text)]\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize_spacy, include_lengths=True, unk_token='<unk>')\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "metadata": {
        "id": "5ql-67-LWIaD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the liar dataset\n",
        "csv_path_liar_train = os.path.join( '/content', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( '/content', 'liar_dataset', 'test.tsv')\n",
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "\n",
        "df_liar_train = df_liar_train[['label', 'statement']]\n",
        "df_liar_test = df_liar_test[['label', 'statement']]\n",
        "df_liar_train = df_liar_train.dropna()\n",
        "df_liar_test = df_liar_test.dropna()\n",
        "\n",
        "# split the data into train and test sets\n",
        "train, test = train_test_split(df_liar_train, test_size=0.1, random_state=7)\n",
        "\n",
        "# save the train and test sets to csv files\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "xm8QsqfmWkUk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='train.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=[('label', LABEL), ('text', TEXT)]\n",
        ")\n",
        "\n",
        "# split the train data into train and validation sets\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "# build the vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data,\n",
        "                    max_size=MAX_VOCAB_SIZE,\n",
        "                    vectors=\"glove.6B.100d\",\n",
        "                    unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "uItuQNOdWl1T"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "vni3NMpHWplR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "n09ecYgMWq6R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 6\n",
        "N_LAYERS = 4\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.7\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = RNN(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "5zIk-uZNWs74"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the accuracy function\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    return correct.float() / y.shape[0]\n",
        "\n",
        "# define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the evaluation function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(iterator):\n",
        "\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "                acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "                # calculate precision, recall and f1 score\n",
        "                y_pred = predictions.argmax(1, keepdim=True)\n",
        "                y_pred = y_pred.squeeze(1)\n",
        "                y_true = batch.label.long()\n",
        "                #y_true = y_true.squeeze(1)\n",
        "\n",
        "                all_predictions.extend(y_pred.cpu().numpy())\n",
        "                all_labels.extend(batch.label.long().cpu().numpy())\n",
        "\n",
        "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
        "# define the function to calculate the time elapsed\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "kpbQ1ewPWvAd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "        valid_loss, valid_acc, _, _, _ = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'liar-model.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0RS58UQWw6G",
        "outputId": "94fae69b-b58a-460f-ff59-363fe09dd315"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 39.71it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 111.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.772 | Train Acc: 19.09%\n",
            "\t Val. Loss: 1.752 |  Val. Acc: 21.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 43.07it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 115.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.765 | Train Acc: 19.51%\n",
            "\t Val. Loss: 1.752 |  Val. Acc: 18.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 42.84it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 105.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.762 | Train Acc: 19.61%\n",
            "\t Val. Loss: 1.751 |  Val. Acc: 19.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 40.34it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 112.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.762 | Train Acc: 20.51%\n",
            "\t Val. Loss: 1.749 |  Val. Acc: 21.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 42.61it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 112.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.760 | Train Acc: 20.70%\n",
            "\t Val. Loss: 1.749 |  Val. Acc: 18.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 42.54it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 112.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.757 | Train Acc: 20.61%\n",
            "\t Val. Loss: 1.747 |  Val. Acc: 21.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 42.24it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 113.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.755 | Train Acc: 21.21%\n",
            "\t Val. Loss: 1.745 |  Val. Acc: 22.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 40.94it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 101.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.751 | Train Acc: 21.62%\n",
            "\t Val. Loss: 1.743 |  Val. Acc: 22.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 41.74it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 111.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.747 | Train Acc: 21.81%\n",
            "\t Val. Loss: 1.745 |  Val. Acc: 22.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101/101 [00:02<00:00, 42.51it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 113.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.747 | Train Acc: 22.39%\n",
            "\t Val. Loss: 1.751 |  Val. Acc: 21.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('liar-model.pt'))\n",
        "\n",
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['RNN'])\n",
        "results.to_csv('results_liar_rnn.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTRbkFpWwz8",
        "outputId": "3c69321f-c324-41bd-dae4-2b92ee9c327b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 52.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.741 | Test Acc: 21.68% | Precision: 0.5069426632851797 | Recall: 0.216796875 | F1: 0.15936520913629976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/u/1/folders/1wf7mFLCqQo0t802IDkZKMOinciUwohuR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1x3iMwddFKm",
        "outputId": "1fa4d73e-5e4d-44b0-af1b-5d6f7e202872"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 11UvyoobnRVXsNkjCsRl848mdYN0Yi18K WELFake_Dataset.csv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11UvyoobnRVXsNkjCsRl848mdYN0Yi18K\n",
            "To: /content/WELFake/WELFake_Dataset.csv\n",
            "100% 245M/245M [00:09<00:00, 24.6MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize, include_lengths=True, unk_token='<unk>')\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM0j3eNajHDa",
        "outputId": "486a4acb-30a2-4e5b-bd43-89f0b3f804cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the welfake dataset\n",
        "\n",
        "csv_path_welfake = os.path.join( '/content', 'WELFake', 'WELFake_Dataset.csv')\n",
        "\n",
        "df = pd.read_csv(csv_path_welfake)\n",
        "df = df.drop(['Unnamed: 0', 'title'], axis=1)\n",
        "df.columns = ['text', 'label']\n",
        "df['label'] = df['label'].replace('fake', 0)\n",
        "df['label'] = df['label'].replace('real', 1)\n",
        "df.to_csv('.//welfake.csv', index=False)\n",
        "# drop the rows with np.nan values on text column\n",
        "df = df.dropna(subset=['text'])\n",
        "df = df[df['text'].str.len() > 30]\n",
        "\n",
        "# split the dataset into train, validation and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# save the train, validation and test sets as csv files\n",
        "train_df.to_csv('.//welfake_train.csv', index=False)\n",
        "test_df.to_csv('.//welfake_test.csv', index=False)"
      ],
      "metadata": {
        "id": "w0HS4aNfdXB4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='welfake_train.csv',\n",
        "    test='welfake_test.csv',\n",
        "    format='csv',\n",
        "    fields=[('text', TEXT), ('label', LABEL)]\n",
        ")\n",
        "\n",
        "# split the train data into train and validation sets\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "metadata": {
        "id": "CdUV8n4gdZk1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data,\n",
        "                    max_size=MAX_VOCAB_SIZE,\n",
        "                    vectors=\"glove.6B.100d\",\n",
        "                    unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "ldXkaZzQdbKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f92999-aa15-4744-bebb-d307e4e3efe5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.42MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18238.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 26\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "lG1hexYWgBon"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 4\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = RNN(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "5zTKn2y4de9f"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the accuracy function\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        # convert tensor to actual text\n",
        "\n",
        "        if any(length <= 0 for length in text_lengths):\n",
        "            print(\"Skipping batch with zero or negative sequence length.\")\n",
        "            continue\n",
        "\n",
        "        #try:\n",
        "        #print(text_lengths)\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        #except:\n",
        "        #  text_debug = [TEXT.vocab.itos[i] for i in text[:,0]]\n",
        "        #  print(text_debug)\n",
        "        #print(text_lengths)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = binary_accuracy(predictions, batch.label.long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the evaluation function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(iterator):\n",
        "\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = criterion(predictions, batch.label)\n",
        "\n",
        "                acc = binary_accuracy(predictions, batch.label.long())\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "                # Convert probability scores to binary predictions using a threshold (e.g., 0.5)\n",
        "                threshold = 0.5\n",
        "                binary_predictions = (predictions > threshold).float()\n",
        "\n",
        "                # calculate precision, recall and f1 score\n",
        "\n",
        "                all_predictions.extend(binary_predictions.cpu().numpy())\n",
        "                all_labels.extend(batch.label.long().cpu().numpy())\n",
        "\n",
        "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
        "# define the function to calculate the time elapsed\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "Kx5aOqJFiotX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "N_EPOCHS = 4\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "            valid_loss, valid_acc, _, _, _ = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                torch.save(model.state_dict(), 'welfake-model.pt')\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "# evaluate the model on the test set\n",
        "\n",
        "model.load_state_dict(torch.load('welfake-model.pt'))\n",
        "\n",
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['RNN'])\n",
        "results.to_csv('results_WELFake_rnn.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCieNlstdRDW",
        "outputId": "caba48ac-871d-487b-e867-44c889dd36c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [07:02<00:00,  3.63it/s]\n",
            "  0%|          | 0/658 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 658/658 [01:10<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 8m 13s\n",
            "\tTrain Loss: 0.118 | Train Acc: 95.14%\n",
            "\t Val. Loss: 0.073 |  Val. Acc: 97.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [07:00<00:00,  3.65it/s]\n",
            "100%|██████████| 658/658 [01:07<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 8m 7s\n",
            "\tTrain Loss: 0.076 | Train Acc: 97.16%\n",
            "\t Val. Loss: 0.040 |  Val. Acc: 98.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [07:01<00:00,  3.64it/s]\n",
            "100%|██████████| 658/658 [01:07<00:00,  9.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 8m 8s\n",
            "\tTrain Loss: 0.044 | Train Acc: 98.42%\n",
            "\t Val. Loss: 0.047 |  Val. Acc: 98.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [07:02<00:00,  3.63it/s]\n",
            "100%|██████████| 658/658 [01:10<00:00,  9.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 8m 13s\n",
            "\tTrain Loss: 0.035 | Train Acc: 98.72%\n",
            "\t Val. Loss: 0.029 |  Val. Acc: 98.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/548 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 548/548 [00:59<00:00,  9.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.034 | Test Acc: 98.77% | Precision: 0.8947368421052632 | Recall: 0.8947368421052632 | F1: 0.8947368421052632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['RNN'])\n",
        "results.to_csv('results_WELFake_rnn.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYxujez_1yEK",
        "outputId": "6532923e-7b76-4174-87f4-eb02cb30d3df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548/548 [00:55<00:00,  9.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.034 | Test Acc: 98.77% | Precision: 0.9882926794849918 | Recall: 0.9882732954146478 | F1: 0.9882390721372001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}