{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUAMt1g8edy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "287ede5d-9857-440e-e1a7-66d6325ddf00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-4.6.0\n",
            "Retrieving folder list\n",
            "Processing file 1hBvEif6hgfiABvVBfM_l9VDhwICnwAXt label_distribution_LIAR.png\n",
            "Processing file 1CcvRkltsFvtSBVsgiqNnj8OSaqPVMdz5 plot.ipynb\n",
            "Processing file 1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C README\n",
            "Processing file 1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG test.tsv\n",
            "Processing file 1RvXY274ln1cKZR6LaUacyytgrrxNqH3K train.tsv\n",
            "Processing file 1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi valid.tsv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hBvEif6hgfiABvVBfM_l9VDhwICnwAXt\n",
            "To: /content/liar_dataset/label_distribution_LIAR.png\n",
            "100% 118k/118k [00:00<00:00, 60.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CcvRkltsFvtSBVsgiqNnj8OSaqPVMdz5\n",
            "To: /content/liar_dataset/plot.ipynb\n",
            "0.00B [00:00, ?B/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YvkO_dP-om5Yh-EJuP2-fljnIEfP-42C\n",
            "To: /content/liar_dataset/README\n",
            "100% 1.67k/1.67k [00:00<00:00, 3.54MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJyzjqaEHUBLzijbjyjpzurPuLaZOSyG\n",
            "To: /content/liar_dataset/test.tsv\n",
            "100% 301k/301k [00:00<00:00, 74.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RvXY274ln1cKZR6LaUacyytgrrxNqH3K\n",
            "To: /content/liar_dataset/train.tsv\n",
            "100% 2.41M/2.41M [00:00<00:00, 101MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n3TWKuZx4ot2zsFnjTEZTnrrtcsZHCKi\n",
            "To: /content/liar_dataset/valid.tsv\n",
            "100% 302k/302k [00:00<00:00, 34.2MB/s]\n",
            "Download completed\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m901.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.3.2 setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==2.3.5\n",
            "  Downloading spacy-2.3.5.tar.gz (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (3.0.9)\n",
            "Collecting thinc<7.5.0,>=7.4.1 (from spacy==2.3.5)\n",
            "  Using cached thinc-7.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.5)\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.5)\n",
            "  Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.5)\n",
            "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (4.66.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (69.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (1.23.5)\n",
            "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.3.5)\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==2.3.5) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2023.11.17)\n",
            "Using cached srsly-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (369 kB)\n",
            "Building wheels for collected packages: spacy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for spacy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for spacy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build spacy\n",
            "\u001b[31mERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m2024-01-29 09:24:52.127245: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 09:24:52.127293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 09:24:52.128636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 09:24:52.135752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 09:24:53.360127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 09:24:55.169930: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 09:24:55.170380: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 09:24:55.170568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown==4.6.0\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/u/1/folders/15Wn46r7gidaiZbx2ArFYsd7rjYH4y7JM\n",
        "\n",
        "!pip install torchtext==0.6.0\n",
        "\n",
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "!pip install -U spacy==2.3.5\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import datetime\n",
        "import spacy\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "LR5d2svJemAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize_spacy(text):\n",
        "    return [token.text for token in nlp(text)]\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize_spacy, include_lengths=True, batch_first=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "metadata": {
        "id": "oSkss_9zeu7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the liar dataset\n",
        "csv_path_liar_train = os.path.join( '/content', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( '/content', 'liar_dataset', 'test.tsv')\n",
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "\n",
        "df_liar_train = df_liar_train[['label', 'statement']]\n",
        "df_liar_test = df_liar_test[['label', 'statement']]\n",
        "df_liar_train = df_liar_train.dropna()\n",
        "df_liar_test = df_liar_test.dropna()\n",
        "\n",
        "# save the train and test sets to csv files\n",
        "df_liar_train.to_csv('train.csv', index=False)\n",
        "df_liar_test.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "_ZskD4QaewMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "id": "csu14HZ0ewzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "UHoB7idKezBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, output_dim, dropout_prob=0.5):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # Calculate total number of filters\n",
        "        total_filters = num_filters * len(filter_sizes)\n",
        "\n",
        "        self.fc = nn.Linear(total_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Permute the tensor to have channels as the second dimension\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        # Apply convolutional layers and use ReLU activation\n",
        "        conv_outputs = [F.relu(conv(embedded)) for conv in self.conv_layers]\n",
        "\n",
        "        # Max-pooling over time\n",
        "        pooled_outputs = [F.max_pool1d(conv_output, conv_output.size(2)).squeeze(2) for conv_output in conv_outputs]\n",
        "\n",
        "        # Concatenate the pooled features\n",
        "        cat_features = torch.cat(pooled_outputs, dim=1)\n",
        "\n",
        "        # Fully connected layer without ReLU in the last layer\n",
        "        fc_output = self.fc(cat_features)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        output = self.dropout(fc_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "gXaENiiFfBzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 64\n",
        "N_FILTERS = 28\n",
        "FILTER_SIZES = [2,3,4]\n",
        "OUTPUT_DIM = 6\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = CNN(25000, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "_KgETxLbfJsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the accuracy function\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    return correct.float() / y.shape[0]\n",
        "\n",
        "# define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the evaluation function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(iterator):\n",
        "\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = model(text).squeeze(1)\n",
        "\n",
        "                loss = criterion(predictions, batch.label.long())\n",
        "\n",
        "                acc = categorical_accuracy(predictions, batch.label.long())\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "                # calculate precision, recall and f1 score\n",
        "                y_pred = predictions.argmax(1, keepdim=True)\n",
        "                y_pred = y_pred.squeeze(1)\n",
        "                y_true = batch.label.long()\n",
        "                #y_true = y_true.squeeze(1)\n",
        "\n",
        "                all_predictions.extend(y_pred.cpu().numpy())\n",
        "                all_labels.extend(batch.label.long().cpu().numpy())\n",
        "\n",
        "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
        "# define the function to calculate the time elapsed\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "5bVWl7ye9bVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "        valid_loss, valid_acc, _, _, _ = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'liar-model.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6uG60Zy9gUC",
        "outputId": "d0e6341f-8e9d-49c0-8776-d175fe403a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 44.96it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 250.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.741 | Train Acc: 24.94%\n",
            "\t Val. Loss: 1.767 |  Val. Acc: 21.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 44.12it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 172.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.701 | Train Acc: 28.45%\n",
            "\t Val. Loss: 1.762 |  Val. Acc: 22.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:03<00:00, 32.65it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 268.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.651 | Train Acc: 31.82%\n",
            "\t Val. Loss: 1.765 |  Val. Acc: 21.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 45.64it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 247.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.611 | Train Acc: 34.43%\n",
            "\t Val. Loss: 1.770 |  Val. Acc: 21.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 47.06it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 256.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.563 | Train Acc: 37.64%\n",
            "\t Val. Loss: 1.770 |  Val. Acc: 20.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 38.80it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 93.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.509 | Train Acc: 40.35%\n",
            "\t Val. Loss: 1.778 |  Val. Acc: 21.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:06<00:00, 16.61it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 242.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 1.461 | Train Acc: 42.13%\n",
            "\t Val. Loss: 1.792 |  Val. Acc: 21.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 45.05it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 261.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.398 | Train Acc: 45.77%\n",
            "\t Val. Loss: 1.794 |  Val. Acc: 22.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 46.54it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 249.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.334 | Train Acc: 48.28%\n",
            "\t Val. Loss: 1.808 |  Val. Acc: 21.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:02<00:00, 37.99it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 174.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 1.278 | Train Acc: 50.35%\n",
            "\t Val. Loss: 1.813 |  Val. Acc: 22.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('liar-model.pt'))\n",
        "\n",
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['CNN'])\n",
        "results.to_csv('results_liar_cnn.csv')"
      ],
      "metadata": {
        "id": "-PB_sd3L9im0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/u/1/folders/1wf7mFLCqQo0t802IDkZKMOinciUwohuR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5rNUpS8neN",
        "outputId": "ca517e1e-aa49-4d48-a5e1-2c5bdd0d5b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 11UvyoobnRVXsNkjCsRl848mdYN0Yi18K WELFake_Dataset.csv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11UvyoobnRVXsNkjCsRl848mdYN0Yi18K\n",
            "To: /content/WELFake/WELFake_Dataset.csv\n",
            "100% 245M/245M [00:04<00:00, 51.5MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "nltk.download('punkt')\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize, include_lengths=True, unk_token='<unk>', batch_first=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjqMpTSV8qoK",
        "outputId": "639a3b29-43f4-42ca-f6c5-027c205d38c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the welfake dataset\n",
        "\n",
        "csv_path_welfake = os.path.join( '/content', 'WELFake', 'WELFake_Dataset.csv')\n",
        "\n",
        "df = pd.read_csv(csv_path_welfake)\n",
        "df = df.drop(['Unnamed: 0', 'title'], axis=1)\n",
        "df.columns = ['text', 'label']\n",
        "df['label'] = df['label'].replace('fake', 0)\n",
        "df['label'] = df['label'].replace('real', 1)\n",
        "df.to_csv('.//welfake.csv', index=False)\n",
        "# drop the rows with np.nan values on text column\n",
        "df = df.dropna(subset=['text'])\n",
        "df = df[df['text'].str.len() > 30]\n",
        "\n",
        "# split the dataset into train, validation and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# save the train, validation and test sets as csv files\n",
        "train_df.to_csv('.//welfake_train.csv', index=False)\n",
        "test_df.to_csv('.//welfake_test.csv', index=False)"
      ],
      "metadata": {
        "id": "d4A7KCys8ubL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='welfake_train.csv',\n",
        "    test='welfake_test.csv',\n",
        "    format='csv',\n",
        "    fields=[('text', TEXT), ('label', LABEL)]\n",
        ")\n",
        "\n",
        "# split the train data into train and validation sets\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "metadata": {
        "id": "iuw4hCE78xeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data,\n",
        "                    max_size=MAX_VOCAB_SIZE,\n",
        "                    vectors=\"glove.6B.100d\",\n",
        "                    unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "svymPEqr8z1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 26\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "_0irKnaT989s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 26\n",
        "N_FILTERS = 28\n",
        "FILTER_SIZES = [2,3,4]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "cPMhfYkK-I3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the accuracy function\n",
        "def binary_accuracy(preds, y):\n",
        "    threshold = .5\n",
        "    binary_preds = (torch.sigmoid(preds) > threshold).float()\n",
        "\n",
        "    correct = (binary_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text)\n",
        "\n",
        "        loss = criterion(predictions, batch.label.unsqueeze(1))\n",
        "\n",
        "        acc = binary_accuracy(predictions, batch.label.unsqueeze(1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# define the evaluation function\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in tqdm(iterator):\n",
        "\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = model(text)\n",
        "\n",
        "                loss = criterion(predictions, batch.label.unsqueeze(1))\n",
        "\n",
        "                acc = binary_accuracy(predictions, batch.label.unsqueeze(1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "                # Convert probability scores to binary predictions using a threshold (e.g., 0.5)\n",
        "                threshold = 0.5\n",
        "                binary_predictions = (predictions > threshold).float()\n",
        "\n",
        "                # calculate precision, recall and f1 score\n",
        "\n",
        "                all_predictions.extend(binary_predictions.cpu().numpy())\n",
        "                all_labels.extend(batch.label.long().cpu().numpy())\n",
        "\n",
        "        precision = precision_score(all_labels, all_predictions, zero_division=True)\n",
        "        recall = recall_score(all_labels, all_predictions, zero_division=True)\n",
        "        f1 = f1_score(all_labels, all_predictions)\n",
        "\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
        "# define the function to calculate the time elapsed\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "        return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "Y9JMypqp-Kzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "N_EPOCHS = 4\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "            valid_loss, valid_acc, _, _, _ = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                torch.save(model.state_dict(), 'welfake-model.pt')\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "# evaluate the model on the test set\n",
        "\n",
        "model.load_state_dict(torch.load('welfake-model.pt'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ-oNQXt-Mxy",
        "outputId": "65c3961d-4490-42a2-9c84-8ff034ac82ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [00:41<00:00, 37.03it/s]\n",
            "100%|██████████| 658/658 [00:07<00:00, 90.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 48s\n",
            "\tTrain Loss: 0.352 | Train Acc: 75.38%\n",
            "\t Val. Loss: 0.045 |  Val. Acc: 98.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [00:41<00:00, 36.80it/s]\n",
            "100%|██████████| 658/658 [00:07<00:00, 86.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 49s\n",
            "\tTrain Loss: 0.350 | Train Acc: 75.58%\n",
            "\t Val. Loss: 0.046 |  Val. Acc: 98.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [00:42<00:00, 36.42it/s]\n",
            "100%|██████████| 658/658 [00:07<00:00, 87.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 49s\n",
            "\tTrain Loss: 0.350 | Train Acc: 75.47%\n",
            "\t Val. Loss: 0.041 |  Val. Acc: 98.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1534/1534 [00:41<00:00, 36.57it/s]\n",
            "100%|██████████| 658/658 [00:07<00:00, 86.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 49s\n",
            "\tTrain Loss: 0.348 | Train Acc: 75.58%\n",
            "\t Val. Loss: 0.039 |  Val. Acc: 98.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548/548 [00:06<00:00, 83.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.037 | Test Acc: 98.71% | Precision: 0.9866243845061924 | Recall: 0.9865178007162418 | F1: 0.9864807774338483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['CNN'])\n",
        "results.to_csv('results_WELFake_cnn.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJtqhH29PWFt",
        "outputId": "3b654015-ea06-4bc0-a919-aaa818c1d922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548/548 [00:06<00:00, 85.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.037 | Test Acc: 98.71% | Precision: 0.9866243845061924 | Recall: 0.9865178007162418 | F1: 0.9864807774338483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize_spacy(text):\n",
        "    return [token.text for token in nlp(text)]\n",
        "\n",
        "\n",
        "# define the fields\n",
        "TEXT = data.Field(tokenize=tokenize_spacy, include_lengths=True, batch_first=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "metadata": {
        "id": "hdMU4Tmlh8IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the liar dataset\n",
        "csv_path_liar_train = os.path.join( '/content', 'liar_dataset', 'train.tsv')\n",
        "csv_path_liar_test = os.path.join( '/content', 'liar_dataset', 'test.tsv')\n",
        "\n",
        "df_liar_train = pd.read_csv(csv_path_liar_train, sep='\\t', header=None)\n",
        "df_liar_test = pd.read_csv(csv_path_liar_test, sep='\\t', header=None)\n",
        "\n",
        "df_liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "df_liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire', 'context']\n",
        "\n",
        "df_liar_train = df_liar_train[['label', 'statement']]\n",
        "df_liar_test = df_liar_test[['label', 'statement']]\n",
        "df_liar_train = df_liar_train.dropna()\n",
        "df_liar_test = df_liar_test.dropna()\n",
        "\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['mostly-true'], 'true')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['half-true'], 'true')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['half-true'], 'true')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['barely-true'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['barely-true'], 'false')\n",
        "df_liar_train['label'] = df_liar_train['label'].replace(['pants-fire'], 'false')\n",
        "df_liar_test['label'] = df_liar_test['label'].replace(['pants-fire'], 'false')\n",
        "\n",
        "# save the train and test sets to csv files\n",
        "df_liar_train.to_csv('train.csv', index=False)\n",
        "df_liar_test.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "SIp_b2xliEdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='train.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=[('label', LABEL), ('text', TEXT)]\n",
        ")\n",
        "\n",
        "# split the train data into train and validation sets\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "# build the vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data,\n",
        "                    max_size=MAX_VOCAB_SIZE,\n",
        "                    vectors=\"glove.6B.100d\",\n",
        "                    unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8f9MomTiHDR",
        "outputId": "e726ce7c-ee64-4d89-cf1f-8c1a387a0968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.41MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18740.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "Z65Zx5APkCUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 64\n",
        "N_FILTERS = 28\n",
        "FILTER_SIZES = [2,3,4]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.6\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# initialize the model\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "# define the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-6)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "# push the model to the device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "J_HpYeroiYc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "            valid_loss, valid_acc, _, _, _ = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                torch.save(model.state_dict(), 'LIAR_BINARY.pt')\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "# evaluate the model on the test set\n",
        "\n",
        "model.load_state_dict(torch.load('LIAR_BINARY.pt'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnKuMYq3idQP",
        "outputId": "cc637458-31ad-4f86-810e-503501c85921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 205.18it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 525.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.702 | Train Acc: 54.35%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 52.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 207.91it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 364.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.704 | Train Acc: 53.79%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 52.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 136.47it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 163.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.702 | Train Acc: 54.70%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:01<00:00, 87.81it/s] \n",
            "100%|██████████| 48/48 [00:00<00:00, 252.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.702 | Train Acc: 54.67%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 132.57it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 298.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.700 | Train Acc: 54.60%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:01<00:00, 111.05it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 361.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.700 | Train Acc: 54.77%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 165.93it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 343.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.702 | Train Acc: 54.69%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 173.38it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 521.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.703 | Train Acc: 54.39%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 220.46it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 519.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.700 | Train Acc: 54.72%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:00<00:00, 225.48it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 525.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.702 | Train Acc: 54.46%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 52.21%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, precision, recall, f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Precision: {precision} | Recall: {recall} | F1: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "# save the results to a csv file\n",
        "results = pd.DataFrame([[test_acc, precision, recall, f1]], columns=['accuracy', 'precision', 'recall', 'f1'], index=['CNN'])\n",
        "results.to_csv('results_LIAR_BINARY_cnn.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPmIW7idinzi",
        "outputId": "e7a3260a-b796-4612-d174-b1765a0417c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 391.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.690 | Test Acc: 52.90% | Precision: 1.0 | Recall: 0.0018083182640144665 | F1: 0.003610108303249097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}